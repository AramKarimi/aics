

@article{Agrawal:2017aa,
  author =        {Aishwarya Agrawal and Dhruv Batra and Devi Parikh and
                   Aniruddha Kembhavi},
  journal =       {arXiv},
  pages =         {1--15},
  title =         {Don't Just Assume; Look and Answer: Overcoming Priors
                   for Visual Question Answering},
  volume =        {arXiv:1712.00377 [cs.CV]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1712.00377},
}

@mastersthesis{Akram:2017aa,
  address =       {Stockholm, Sweden},
  author =        {Saad Ullah Akram},
  school =        {School of Computer Science and Communication, Control
                   and Robotics, Royal Institute of Technolog},
  title =         {Visual recognition of isolated Swedish sign language
                   signs},
  year =          {2012},
}

@phdthesis{Anderson:2018aa,
  author =        {Peter Anderson},
  month =         {April},
  school =        {The Australian National University},
  type =          {A thesis submitted for the degree of Doctor of
                   Philosophy},
  title =         {Vision and Language Learning: From Image Captioning
                   and Visual Question Answering towards Embodied
                   Agents},
  year =          {2018},
  url =           {https://panderson.me/images/Andersonthesis.pdf},
}

@inproceedings{Anderson:2018ab,
  author =        {Anderson, Peter and He, Xiaodong and Buehler, Chris and
                   Teney, Damien and Johnson, Mark and Gould, Stephen and
                   Zhang, Lei},
  booktitle =     {The IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  month =         {June},
  title =         {Bottom-Up and Top-Down Attention for Image Captioning
                   and Visual Question Answering},
  year =          {2018},
  url =           {http://openaccess.thecvf.com/content_cvpr_2018/html/
                  Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html},
}

@article{Andreas:2015aa,
  author =        {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and
                   Dan Klein},
  journal =       {arXiv},
  title =         {Deep Compositional Question Answering with Neural
                   Module Networks},
  volume =        {arXiv:1511.02799 [cs.CV]},
  year =          {2015},
  url =           {http://arxiv.org/abs/1511.02799},
}

@inproceedings{Andreas:2016aa,
  address =       {San Diego, California},
  author =        {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and
                   Dan Klein},
  booktitle =     {Proceedings of {NAACL-HLT} 2016},
  journal =       {CoRR},
  month =         {June 12-17},
  organization =  {Association for Computational Linguistics},
  pages =         {1545--1554},
  title =         {Learning to Compose Neural Networks for Question
                   Answering},
  year =          {2016},
  url =           {http://arxiv.org/abs/1601.01705},
}

@inproceedings{Antol:2015wh,
  author =        {Antol, Stanislaw and Agrawal, Aishwarya and
                   Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and
                   Zitnick, C. Lawrence and Parikh, Devi},
  booktitle =     {Proceedings of the IEEE International Conference on
                   Computer Vision (ICCV)},
  month =         {December},
  title =         {{VQA}: {V}isual {Q}uestion {A}nswering},
  year =          {2015},
  abstract =      {We propose the task of free-form and open-ended
                   Visual Question Answering (VQA). Given an image and a
                   natural language question about the image, the task
                   is to provide an accurate natural language answer.
                   Mirroring real-world scenarios, such as helping the
                   visually impaired, both the questions and answers are
                   open-ended. Visual questions selectively target
                   different areas of an image, including background
                   details and underlying context. As a result, a system
                   that succeeds at VQA typically needs a more detailed
                   understanding of the image and complex reasoning than
                   a system producing generic image captions. Moreover,
                   VQA is amenable to automatic evaluation, since many
                   open-ended answers contain only a few words or a
                   closed set of answers that can be provided in a
                   multiple-choice format. We provide a dataset
                   containing 0.25M images, 0.76M questions, and 10M
                   answers (www.visualqa.org), and discuss the
                   information it provides. Numerous baselines for VQA
                   are provided and compared with human performance.},
}

@techreport{Artzi:2020aa,
  address =       {ACL 2020 Workshop on Advances in Language and Vision
                   Research (ALVR)},
  author =        {Artzi, Yoav},
  institution =   {Cornell University},
  month =         {July 9},
  type =          {Invited talk},
  title =         {Robust control in situated instruction following},
  year =          {2020},
  url =           {https://slideslive.com/38929761/robot-control-in-situated-
                  instruction-following},
}

@techreport{Astbom:2017aa,
  address =       {Gothenburg, Sweden},
  author =        {{\AA}stbom, Amelie},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {February 7},
  note =          {{S}upervisor: Simon Dobnik, opponent: Linnea Strand,
                   examiner: Christine Howes},
  type =          {C-uppsats (Bachelor's thesis/extended essay)},
  title =         {How function of objects affects geometry of spatial
                   descriptions. {A} study of {Swedish} and {Japanese}},
  year =          {2017},
}

@article{Baron-Cohen:1985aa,
  author =        {Simon Baron-Cohen and Alan M. Leslie and Uta Frith},
  journal =       {Cognition},
  month =         {October},
  number =        {1},
  pages =         {37--46},
  title =         {Does the autistic child have a ``theory of mind'' ?},
  volume =        {21},
  year =          {1985},
  url =           {https://doi.org/10.1016/0010-0277(85)90022-8},
}

@article{Barsalou:1999uq,
  author =        {Lawrence W. Barsalou},
  journal =       {Behavioral and Brain Sciences},
  pages =         {577--609},
  title =         {Perceptual symbol systems},
  volume =        {22},
  year =          {1999},
}

@article{Barsalou:2008aa,
  author =        {Barsalou, Lawrence W.},
  journal =       {Annual Review of Psychology},
  pages =         {617--645},
  title =         {Grounded cognition},
  volume =        {59},
  year =          {2008},
  abstract =      {Grounded cognition rejects traditional views that
                   cognition is computation on amodal symbols in a
                   modular system, independent of the brain's modal
                   systems for perception, action, and introspection.
                   Instead, grounded cognition proposes that modal
                   simulations, bodily states, and situated action
                   underlie cognition. Accumulating behavioral and
                   neural evidence supporting this view is reviewed from
                   research on perception, memory, knowledge, language,
                   thought, social cognition, and development. Theories
                   of grounded cognition are also reviewed, as are
                   origins of the area and common misperceptions of it.
                   Theoretical, empirical, and methodological issues are
                   raised whose future treatment is likely to affect the
                   growth and impact of grounded cognition.},
  doi =           {10.1146/annurev.psych.59.103006.093639},
  url =           {https://doi.org/10.1146/annurev.psych.59.103006.093639},
}

@article{Bateman:2019aa,
  author =        {Bateman, John A. and Pomarlan, Mihai and
                   Kazhoyan, Gayane},
  journal =       {Applied Ontology},
  month =         {2 October},
  pages =         {1--35},
  title =         {Embodied contextualization: Towards a multistratal
                   ontological treatment},
  volume =        {Pre-press},
  year =          {2019},
  doi =           {10.3233/AO-190218},
  url =           {https://content.iospress.com/articles/applied-ontology/
                  ao190218},
}

@article{Battaglia:2013aa,
  author =        {Battaglia, Peter W. and Hamrick, Jessica B. and
                   Tenenbaum, Joshua B.},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {45},
  pages =         {18327-18332},
  title =         {Simulation as an engine of physical scene
                   understanding},
  volume =        {110},
  year =          {2013},
  abstract =      {In a glance, we can perceive whether a stack of
                   dishes will topple, a branch will support a child's
                   weight, a grocery bag is poorly packed and liable to
                   tear or crush its contents, or a tool is firmly
                   attached to a table or free to be lifted. Such rapid
                   physical inferences are central to how people
                   interact with the world and with each other, yet
                   their computational underpinnings are poorly
                   understood. We propose a model based on an
                   ``intuitive physics engine,'' a cognitive mechanism
                   similar to computer engines that simulate rich
                   physics in video games and graphics, but that uses
                   approximate, probabilistic simulations to make robust
                   and fast inferences in complex natural scenes where
                   crucial information is unobserved. This single model
                   fits data from five distinct psychophysical tasks,
                   captures several illusions and biases, and explains
                   core aspects of human mental models and common-sense
                   reasoning that are instrumental to how humans
                   understand their everyday world.},
  doi =           {10.1073/pnas.1306572110},
  url =           {http://www.pnas.org/content/110/45/18327.abstract},
}

@inproceedings{Baumgaertner:2012wm,
  address =       {Montr{\'e}al, Canada},
  author =        {Baumgaertner, Bert and Fern{\'a}ndez, Raquel and
                   Stone, Matthew},
  booktitle =     {*{SEM} 2012: The First Joint Conference on Lexical
                   and Computational Semantics {--} Volume 1:
                   Proceedings of the main conference and the shared
                   task, and Volume 2: Proceedings of the Sixth
                   International Workshop on Semantic Evaluation
                   ({S}em{E}val 2012)},
  month =         {7-8 } # jun,
  pages =         {80--84},
  publisher =     {Association for Computational Linguistics},
  title =         {Towards a Flexible Semantics: Colour Terms in
                   Collaborative Reference Tasks},
  year =          {2012},
  url =           {https://www.aclweb.org/anthology/S12-1013},
}

@article{Bernardi:2016aa,
  author =        {Bernardi, Raffaella and Cakici, Ruket and
                   Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and
                   Ikizler-Cinbis, Nazli and Keller, Frank and
                   Muscat, Adrian and Plank, Barbara},
  journal =       {Journal of Artificial Intelligence Research},
  pages =         {409--442},
  title =         {Automatic description generation from images: A
                   survey of models, datasets, and evaluation measures},
  volume =        {55},
  year =          {2016},
  doi =           {https://doi.org/10.1613/jair.4900},
}

@book{Bradski:2008aa,
  author =        {Bradski, Gary and Kaehler, Adrian},
  publisher =     {O'Reilly Media, Inc.},
  title =         {Learning {OpenCV}: Computer vision with the {OpenCV}
                   library},
  year =          {2008},
}

@article{Breazeal:2006aa,
  author =        {Cynthia Breazeal and Matt Berlin and Andrew Brooks and
                   Jesse Gray and Andrea L. Thomaz},
  journal =       {Robotics and Autonomous Systems},
  number =        {5},
  pages =         {385--393},
  title =         {Using perspective taking to learn from ambiguous
                   demonstrations},
  volume =        {54},
  year =          {2006},
  abstract =      {This paper addresses an important issue in learning
                   from demonstrations that are provided by
                   ``na{\"\i}ve'' human teachers---people who do not
                   have expertise in the machine learning algorithms
                   used by the robot. We therefore entertain the
                   possibility that, whereas the average human user may
                   provide sensible demonstrations from a human's
                   perspective, these same demonstrations may be
                   insufficient, incomplete, ambiguous, or otherwise
                   ``flawed'' from the perspective of the training set
                   needed by the learning algorithm to generalize
                   properly. To address this issue, we present a system
                   where the robot is modeled as a socially engaged and
                   socially cognitive learner. We illustrate the merits
                   of this approach through an example where the robot
                   is able to correctly learn from ``flawed''
                   demonstrations by taking the visual perspective of
                   the human instructor to clarify potential
                   ambiguities.},
  doi =           {http://dx.doi.org/10.1016/j.robot.2006.02.004},
  issn =          {0921-8890},
}

@techreport{Brownlee:2019aa,
  author =        {Brownlee, Jason},
  institution =   {Machine Learning Mastery},
  month =         {June 17},
  type =          {Blogpost article},
  title =         {A Gentle Introduction to Generative Adversarial
                   Networks ({GANs})},
  year =          {2019},
  url =           {https://machinelearningmastery.com/what-are-generative-
                  adversarial-networks-gans/},
}

@article{Bruni:2014aa,
  author =        {Bruni, Elia and Tran, Nam-Khanh and Baroni, Marco},
  journal =       {Journal of Artificial Intelligence Research (JAIR)},
  number =        {1--47},
  title =         {Multimodal Distributional Semantics},
  volume =        {49},
  year =          {2014},
}

@inproceedings{Byron:2003aa,
  author =        {Byron, Donna K},
  booktitle =     {Proceedings of the First International Workshop on
                   Language Understanding and Agents for Real World
                   Interaction},
  pages =         {39--47},
  title =         {Understanding referring expressions in situated
                   language some challenges for real-world agents},
  year =          {2003},
  url =           {https://citeseerx.ist.psu.edu/viewdoc/download?
                  doi=10.1.1.134.1408&rep=rep1&type=pdf},
}

@inproceedings{Cadene:2019vb,
  author =        {Cadene, Remi and Dancette, Corentin and
                   Cord, Matthieu and Parikh, Devi},
  booktitle =     {NeurIPS},
  pages =         {841--852},
  title =         {{RUBi}: Reducing Unimodal Biases for Visual Question
                   Answering},
  year =          {2019},
  url =           {https://arxiv.org/pdf/1906.10169.pdf},
}

@inproceedings{Can:2019aa,
  address =       {Minneapolis, Minnesota},
  author =        {Can, Ozan Arkan and Zuidberg Dos Martires, Pedro and
                   Persson, Andreas and Gaal, Julian and Loutfi, Amy and
                   De Raedt, Luc and Yuret, Deniz and
                   Saffiotti, Alessandro},
  booktitle =     {Proceedings of the Combined Workshop on Spatial
                   Language Understanding ({S}p{LU}) and Grounded
                   Communication for Robotics ({R}obo{NLP})},
  month =         jun,
  pages =         {29--39},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning from Implicit Information in Natural
                   Language Instructions for Robotic Manipulations},
  year =          {2019},
  abstract =      {Human-robot interaction often occurs in the form of
                   instructions given from a human to a robot. For a
                   robot to successfully follow instructions, a common
                   representation of the world and objects in it should
                   be shared between humans and the robot so that the
                   instructions can be grounded. Achieving this
                   representation can be done via learning, where both
                   the world representation and the language grounding
                   are learned simultaneously. However, in robotics this
                   can be a difficult task due to the cost and scarcity
                   of data. In this paper, we tackle the problem by
                   separately learning the world representation of the
                   robot and the language grounding. While this approach
                   can address the challenges in getting sufficient
                   data, it may give rise to inconsistencies between
                   both learned components. Therefore, we further
                   propose Bayesian learning to resolve such
                   inconsistencies between the natural language
                   grounding and a robot{'}s world representation by
                   exploiting spatio-relational information that is
                   implicitly present in instructions given by a human.
                   Moreover, we demonstrate the feasibility of our
                   approach on a scenario involving a robotic arm in the
                   physical world.},
  url =           {https://www.aclweb.org/anthology/W19-1604},
}

@article{Cangelosi:2010aa,
  author =        {A. {Cangelosi} and G. {Metta} and G. {Sagerer} and
                   S. {Nolfi} and C. {Nehaniv} and K. {Fischer} and
                   J. {Tani} and T. {Belpaeme} and G. {Sandini} and
                   F. {Nori} and L. {Fadiga} and B. {Wrede} and
                   K. {Rohlfing} and E. {Tuci} and K. {Dautenhahn} and
                   J. {Saunders} and A. {Zeschel}},
  journal =       {IEEE Transactions on Autonomous Mental Development},
  month =         {September},
  number =        {3},
  pages =         {167-195},
  title =         {Integration of Action and Language Knowledge: A
                   Roadmap for Developmental Robotics},
  volume =        {2},
  year =          {2010},
  abstract =      {This position paper proposes that the study of
                   embodied cognitive agents, such as humanoid robots,
                   can advance our understanding of the cognitive
                   development of complex sensorimotor, linguistic, and
                   social learning skills. This in turn will benefit the
                   design of cognitive robots capable of learning to
                   handle and manipulate objects and tools autonomously,
                   to cooperate and communicate with other robots and
                   humans, and to adapt their abilities to changing
                   internal, environmental, and social conditions. Four
                   key areas of research challenges are discussed,
                   specifically for the issues related to the
                   understanding of: 1) how agents learn and represent
                   compositional actions; 2) how agents learn and
                   represent compositional lexica; 3) the dynamics of
                   social interaction and learning; and 4) how
                   compositional action and language representations are
                   integrated to bootstrap the cognitive system. The
                   review of specific issues and progress in these areas
                   is then translated into a practical roadmap based on
                   a series of milestones. These milestones provide a
                   possible set of cognitive robotics goals and test
                   scenarios, thus acting as a research roadmap for
                   future work on cognitive developmental robotics.},
  doi =           {10.1109/TAMD.2010.2053034},
  issn =          {1943-0604},
}

@techreport{Cano-Santin:2019ab,
  address =       {Gothenburg, Sweden},
  author =        {Cano Sant{\'\i}n, Jos{\'e} Miguel},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {September 18},
  note =          {{S}upervisor: Simon Dobnik and Mehdi Ghanimifard,
                   examiner: Aarne Ranta},
  type =          {Masters in Language Technology (MLT), 30 HEC},
  title =         {Fast visual grounding in interaction: bringing
                   few-shot learning with neural networks to an
                   interactive robot},
  year =          {2019},
  url =           {http://hdl.handle.net/2077/62035},
}

@inproceedings{Cano-Santin:2020aa,
  address =       {Gothenburg, Sweden},
  author =        {Cano Sant{\'\i}n, Jos{\'e} Miguel and Dobnik, Simon and
                   Ghanimifard, Mehdi},
  booktitle =     {Proceedings of Conference on Probability and Meaning
                   (PaM-2020), Gothenburg, Sweden},
  month =         {October 14--16},
  organization =  {Association for Computational Linguistics (ACL),
                   Special Interest Group on Computational Semantics
                   (SIGSEM)},
  pages =         {1--9},
  title =         {Fast visual grounding in interaction: bringing
                   few-shot learning with neural networks to an
                   interactive robot},
  year =          {2020},
  abstract =      {The major shortcomings of using neural networks with
                   situated agents are that in incremental interaction
                   very few learning examples are available and that
                   their visual sensory representations are quite
                   di!erent from image caption datasets. In this work we
                   adapt and evaluate a few-shot learning approach,
                   Matching Networks (Vinyals et al., 2016), to
                   conversational strategies of a robot interacting with
                   a human tutor in order to e"ciently learn to
                   categorise objects that are presented to it and also
                   investigate to what degree transfer learning from
                   pre-trained models on images from di!erent contexts
                   can improve its performance. We discuss the
                   implications of such learning on the nature of
                   semantic representations the system has learned.
                   situated interactive agents, robots, few-shot
                   learning, object recognition, dialogue, grounding},
  url =           {https://gup.ub.gu.se/publication/294796?lang=en},
}

@article{Cao:2018aa,
  author =        {Kris Cao and Angeliki Lazaridou and Marc Lanctot and
                   Joel Z Leibo and Karl Tuyls and Stephen Clark},
  journal =       {arXiv},
  title =         {Emergent Communication through Negotiation},
  volume =        {arXiv:1804.03980 [cs.AI]},
  year =          {2018},
  url =           {https://arxiv.org/abs/1804.03980},
}

@incollection{Chai:2018aa,
  author =        {Chai, Joyce Y and Cakmak, Maya and Sidner, Candace},
  booktitle =     {Interactive Task Learning: Agents, Robots, and Humans
                   Acquiring New Tasks through Natural Interactions},
  chapter =       {9},
  editor =        {K. A. Cluck and J. E. Laird},
  publisher =     {MIT press},
  series =        {Strungmann Forum Reports},
  title =         {Teaching Robots New Tasks through Natural
                   Interaction},
  year =          {2018},
}

@inproceedings{Chai:2018ab,
  author =        {Joyce Y. Chai and Qiaozi Gao and Lanbo She and
                   Shaohua Yang and Sari Saba-Sadiya and Guangyue Xu},
  booktitle =     {Proceedings of the Twenty-Seventh International Joint
                   Conference on Artificial Intelligence, {IJCAI-18}},
  month =         {7},
  pages =         {2--9},
  publisher =     {International Joint Conferences on Artificial
                   Intelligence Organization},
  title =         {Language to Action: Towards Interactive Task Learning
                   with Physical Agents},
  year =          {2018},
  doi =           {10.24963/ijcai.2018/1},
  url =           {https://doi.org/10.24963/ijcai.2018/1},
}

@article{Chattopadhyay:2017aa,
  author =        {Prithvijit Chattopadhyay and Deshraj Yadav and
                   Viraj Prabhu and Arjun Chandrasekaran and
                   Abhishek Das and Stefan Lee and Dhruv Batra and
                   Devi Parikh},
  journal =       {arXiv},
  pages =         {1--9},
  title =         {Evaluating Visual Conversational Agents via
                   Cooperative Human-AI Games},
  volume =        {arXiv:1708.05122 [cs.HC]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1708.05122},
}

@techreport{Choi:2020aa,
  address =       {MIT Embodied Intelligence Seminar},
  author =        {Choi, Yejin},
  institution =   {Paul G. Allen School of Computer Science and
                   Enginneering, University of Washington and Allen
                   Institute for Artificial Intelligence},
  month =         {November 4},
  type =          {Seminar},
  title =         {Intuitive reasoning as (un)supervised language
                   generation},
  year =          {2020},
  abstract =      {Neural language models, as they grow in scale,
                   continue to surprise us with utterly nonsensical and
                   counterintuitive errors despite their otherwise
                   remarkable performances on leaderboards. In this
                   talk, I will argue that it is time to challenge the
                   currently dominant paradigm of task-specific
                   supervision built on top of large-scale
                   self-supervised neural networks. I will first
                   highlight how we can make better lemonade out of
                   neural language models by shifting our focus on
                   unsupervised, inference-time algorithms. I will
                   demonstrate how unsupervised algorithms can match or
                   even outperform supervised approaches on hard
                   reasoning tasks such as nonmonotonic reasoning (such
                   as counterfactual and abductive reasoning), or
                   complex language generation tasks that require
                   logical constraints. Next, I will highlight the
                   importance of melding explicit and declarative
                   knowledge encoded in symbolic knowledge graphs with
                   implicit and observed knowledge encoded in neural
                   language models. I will present COMET, Commonsense
                   Transformers that learn neural representation of
                   commonsense reasoning from a symbolic commonsense
                   knowledge graph, and Social Chemistry 101, a new
                   conceptual formalism, a knowledge graph, and neural
                   models to reason about social, moral, and ethical
                   norms.},
  url =           {https://youtu.be/h2wzQKRAdA8},
}

@article{Collell:2017aa,
  author =        {Guillem Collell and Luc Van Gool and
                   Marie{-}Francine Moens},
  journal =       {arXiv},
  pages =         {1--8},
  title =         {Acquiring Common Sense Spatial Knowledge through
                   Implicit Spatial Templates},
  volume =        {arXiv:1711.06821 [cs.AI]},
  year =          {2017},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  url =           {http://arxiv.org/abs/1711.06821},
}

@article{Collell:2018aa,
  author =        {Collell, Guillem and Moens, Marie-Francine},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {133--144},
  title =         {Learning Representations Specialized in Spatial
                   Knowledge: Leveraging Language and Vision},
  volume =        {6},
  year =          {2018},
  abstract =      {Spatial understanding is crucial in many real-world
                   problems, yet little progress has been made towards
                   building representations that capture spatial
                   knowledge. Here, we move one step forward in this
                   direction and learn such representations by
                   leveraging a task consisting in predicting continuous
                   2D spatial arrangements of objects given
                   object-relationship-object instances (e.g., ``cat
                   under chair\") and a simple neural network model that
                   learns the task from annotated images. We show that
                   the model succeeds in this task and that it is
                   furthermore capable of predicting correct spatial
                   arrangements for unseen objects if either CNN
                   features or word embeddings of the objects are
                   provided. The differences between visual and
                   linguistic features are discussed. Next, to evaluate
                   the spatial representations learned in the previous
                   task, we introduce a task and a dataset consisting in
                   a set of crowdsourced human ratings of spatial
                   similarity for object pairs. We find that both CNN
                   features and word embeddings predict well human
                   judgments of similarity and that these vectors can be
                   further specialized in spatial knowledge if we update
                   them when training the model that predicts spatial
                   arrangements of objects. Overall, this paper paves
                   the way towards building distributed spatial
                   representations, contributing to the understanding of
                   spatial expressions in language.},
  issn =          {2307-387X},
  url =           {https://www.transacl.org/ojs/index.php/tacl/article/view/
                  1214},
}

@article{Collell:2018ab,
  author =        {Guillem Collell and Marie-Francine Moens},
  journal =       {arXiv},
  pages =         {1--13},
  title =         {Do Neural Network Cross-Modal Mappings Really Bridge
                   Modalities?},
  volume =        {arXiv:1805.07616 [stat.ML]},
  year =          {2018},
}

@unpublished{Cooperinprepa,
  author =        {Robin Cooper},
  note =          {Draft at
  \url{https://sites.google.com/site/typetheorywithrecords/drafts}},
  title =         {From perception to communication: An analysis of
                   meaning and action using a theory of types with
                   records {(TTR)}},
  year =          {in prep},
  url =           {https://sites.google.com/site/typetheorywithrecords/drafts},
}

@incollection{Coventry:2005aa,
  author =        {Coventry, Kenny R. and Cangelosi, Angelo and
                   Rajapakse, Rohanna and Bacon, Alison and
                   Newstead, Stephen and Joyce, Dan and
                   Richards, Lynn V.},
  booktitle =     {Spatial Cognition IV. Reasoning, Action, Interaction},
  editor =        {Freksa, Christian and Knauff, Markus and
                   Krieg-Br{\"u}ckner, Bernd and Nebel, Bernhard and
                   Barkowsky, Thomas},
  pages =         {98-110},
  publisher =     {Springer Berlin Heidelberg},
  series =        {Lecture Notes in Computer Science},
  title =         {Spatial Prepositions and Vague Quantifiers:
                   Implementing the Functional Geometric Framework},
  volume =        {3343},
  year =          {2005},
  doi =           {10.1007/978-3-540-32255-9\_6},
  isbn =          {978-3-540-25048-7},
}

@incollection{Coventry:2005ab,
  author =        {Coventry, Kenny and Garrod, Simon},
  booktitle =     {Functional features in language and space: insights
                   from perception, categorization, and development},
  editor =        {Carlson, Laura Anne and Zee, Emile van der},
  pages =         {149--162},
  publisher =     {Oxford University Press},
  title =         {Spatial prepositions and the functional geometric
                   framework. Towards a classification of
                   extra-geometric influences.},
  volume =        {2},
  year =          {2005},
}

@inproceedings{Das:2017aa,
  author =        {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and
                   Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and
                   Parikh, Devi and Batra, Dhruv},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {326--335},
  title =         {Visual dialog},
  year =          {2017},
}

@inproceedings{Das:2018aa,
  author =        {Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and
                   Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition Workshops},
  pages =         {2054--2063},
  title =         {Embodied question answering},
  year =          {2018},
  url =           {https://openaccess.thecvf.com/content_cvpr_2018/html/
                  Das_Embodied_Question_Answering_CVPR_2018_paper.html},
}

@inproceedings{De-Vries:2017aa,
  author =        {De Vries, Harm and Strub, Florian and Chandar, Sarath and
                   Pietquin, Olivier and Larochelle, Hugo and
                   Courville, Aaron},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  month =         {July},
  pages =         {5503--5512},
  title =         {Guesswhat?! visual object discovery through
                   multi-modal dialogue},
  year =          {2017},
}

@inproceedings{Demirel:2017aa,
  author =        {Demirel, Berkan and Gokberk Cinbis, Ramazan and
                   Ikizler-Cinbis, Nazli},
  booktitle =     {The IEEE International Conference on Computer Vision
                   (ICCV)},
  month =         {October},
  title =         {Attributes2Classname: A Discriminative Model for
                   Attribute-Based Unsupervised Zero-Shot Learning},
  year =          {2017},
}

@article{Dissanayake:2001,
  author =        {Dissanayake, M. W. M. G and Newman, P. M. and
                   Durrant-Whyte, H. F. and Clark, S. and Csorba, M.},
  journal =       {IEEE Transactions on Robotic and Automation},
  number =        {3},
  pages =         {229-241},
  title =         {A solution to the simultaneous localization and map
                   building ({SLAM}) problem},
  volume =        {17},
  year =          {2001},
  doi =           {10.1109/70.938381},
}

@incollection{DiTomasoLombardo:1998,
  address =       {Mahwah, N.J.},
  author =        {{Di Tomaso}, Vittorio and Lombardo, Vecenzo},
  booktitle =     {Representation and Processing of Spatial Expressions},
  editor =        {Olivier, Patrick and Gapp, Klaus-Peter},
  pages =         {73--90},
  publisher =     {Lawrence Erlbaum Associates},
  title =         {A computational model for the interpretation of
                   static locative expressions},
  year =          {1998},
}

@phdthesis{Dobnik:2009dz,
  address =       {Oxford, United Kingdom},
  author =        {Dobnik, Simon},
  month =         {September 4},
  school =        {University of Oxford: Faculty of Linguistics,
                   Philology and Phonetics and The Queen's College},
  title =         {Teaching mobile robots to use spatial words},
  year =          {2009},
  url =           {https://gup.ub.gu.se/publication/270997},
}

@inproceedings{Dobnik:2013aa,
  address =       {Berlin, Germany},
  author =        {Dobnik, Simon and Kelleher, John D.},
  booktitle =     {Proceedings of PRE-CogSci 2013 Production of
                   referring expressions -- bridging the gap between
                   cognitive and computational approaches to reference
                   at CogSci},
  month =         {31 July},
  pages =         {1--6},
  title =         {Towards an automatic identification of functional and
                   geometric spatial prepositions},
  year =          {2013},
  url =           {http://pre2013.uvt.nl/pdf/dobnik-kelleher.pdf},
}

@inproceedings{Dobnik:2014aa,
  address =       {Edinburgh},
  author =        {Dobnik, Simon and Kelleher, John D. and
                   Koniaris, Christos},
  booktitle =     {Proceedings of {DialWatt} -- Semdial 2014: The 18th
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Verena Rieser and Philippe Muller},
  month =         {1--3 September},
  pages =         {43--52},
  title =         {Priming and Alignment of Frame of Reference in
                   Situated Conversation},
  year =          {2014},
  url =           {http://gup.ub.gu.se/records/fulltext/200741/200741.pdf},
}

@inproceedings{Dobnik:2014ab,
  address =       {Dublin, Ireland},
  author =        {Dobnik, Simon and Kelleher, John D.},
  booktitle =     {Proceedings of the Third V\&L Net Workshop on Vision
                   and Language at COLING},
  month =         {August},
  pages =         {33--37},
  publisher =     {Dublin City University and the Association for
                   Computational Linguistics},
  title =         {Exploration of functional semantics of prepositions
                   from corpora of descriptions of visual scenes},
  year =          {2014},
  url =           {http://www.aclweb.org/anthology/W14-5405},
}

@inproceedings{Dobnik:2015aa,
  address =       {Gothenburg, Sweden},
  author =        {Dobnik, Simon and Howes, Christine and
                   Kelleher, John D.},
  booktitle =     {Proceedings of {goDIAL} -- Semdial 2015: The 19th
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Howes, Christine and Larsson, Staffan},
  month =         {24--26th August},
  pages =         {24--32},
  title =         {Changing perspective: Local alignment of reference
                   frames in dialogue},
  year =          {2015},
  abstract =      {In this paper we examine how people negotiate,
                   interpret and repair the frame of reference (FoR) in
                   free dialogues discussing spatial scenes. We describe
                   a pilot study in which participants are given
                   different perspectives of the same scene and asked to
                   locate several objects that are only shown on one of
                   their pictures. This task requires participants to
                   coordinate on FoR in order to identify the missing
                   objects. Preliminary results indicate that
                   conversational participants align locally on FoR but
                   do not converge on a global frame of reference.
                   Misunderstandings lead to clarification sequences in
                   which participants shift the FoR. These findings have
                   implications for situated dialogue systems.},
  url =           {https://gup.ub.gu.se/publication/224188},
}

@inproceedings{Dobnik:2017aa,
  address =       {Gothenburg, Sweden},
  author =        {Dobnik, Simon and de Graaf, Erik},
  booktitle =     {Proceedings of the 21st Nordic Conference on
                   Computational Linguistics (NoDaLiDa)},
  editor =        {Tiedemann, J{\"o}rg and Tahmasebi, Nina},
  month =         {22--24 May},
  organization =  {Northern European Association for Language Technology
                   (NEALT)},
  pages =         {162--171},
  publisher =     {Association for Computational Linguistics},
  title =         {{KILLE}: a Framework for Situated Agents for Learning
                   Language Through Interaction},
  year =          {2017},
  url =           {https://gup.ub.gu.se/publication/253950},
}

@inproceedings{Dobnik:2017ac,
  address =       {Saarbr{\"u}cken, Germany},
  author =        {Dobnik, Simon and {\AA}stbom, Amelie},
  booktitle =     {Proceedings of {Saardial} -- Semdial 2017: The 21st
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Petukhova, Volha and Tian, Ye},
  month =         {August 15--17},
  pages =         {17--26},
  title =         {{(Perceptual)} grounding as interaction},
  year =          {2017},
  url =           {https://gup.ub.gu.se/publication/255455},
}

@inproceedings{Dobnik:2018ab,
  address =       {New Orleans, Louisiana, USA},
  author =        {Dobnik, Simon and Ghanimifard, Mehdi and
                   Kelleher, John D.},
  booktitle =     {Proceedings of the First International Workshop on
                   Spatial Language Understanding {(SpLU 2018)} at
                   {NAACL-HLT 2018}},
  month =         {June 6},
  organization =  {Association for Computational Linguistics},
  pages =         {1--11},
  title =         {Exploring the Functional and Geometric Bias of
                   Spatial Relations Using Neural Language Models},
  year =          {2018},
  abstract =      {The challenge for computational models of spatial
                   descriptions for situated dialogue systems is the
                   integration of information from different modalities.
                   The semantics of spatial descriptions are grounded in
                   at least two sources of information: (i) a geometric
                   representation of space and (ii) the functional
                   interaction of related objects that. We train several
                   neural language models on descriptions of scenes from
                   a dataset of image captions and examine whether the
                   functional or geometric bias of spatial descriptions
                   reported in the literature is reflected in the
                   estimated perplexity of these models. The results of
                   these experiments have implications for the creation
                   of models of spatial lexical semantics for
                   human-robot dialogue systems. Furthermore, they also
                   provide an insight into the kinds of the semantic
                   knowledge captured by neural language models trained
                   on spatial descriptions, which has implications for
                   image captioning systems.},
  url =           {https://gup.ub.gu.se/publication/267064?lang=en},
}

@inproceedings{Dobnik:2019ad,
  address =       {London, UK},
  author =        {Dobnik, Simon and Lo{\'a}iciga, Sharid},
  booktitle =     {Proceedings of {LondonLogue} -- {Semdial} 2019: The
                   23rd Workshop on the Semantics and Pragmatics of
                   Dialogue},
  editor =        {Hough, Julian and Howes, Christine and
                   Kennington, Casey},
  month =         {4--6 September},
  organization =  {Queen Mary University of London},
  pages =         {1--3},
  title =         {On Visual Coreference Chains Resolution},
  year =          {2019},
  abstract =      {We explore to what degree an existing textual
                   coreference resolution tools can be applied to visual
                   dialogue data. The analysis of error of the
                   coreference system (i) demonstrates the extent to
                   which such data differs from the writ- ten document
                   texts that these tools are typ- ically applied on;
                   (ii) informs about the re- lation between information
                   expressed in lan- guage and vision; and (iii)
                   suggests further di- rections in which coreference
                   tools should be adapted for visual dialogue. visual
                   dialogue, coreference resolution, language, vision,
                   coreference tools},
  url =           {https://gup.ub.gu.se/publication/283001?lang=en},
}

@techreport{Dobnik:2019ae,
  address =       {University of Latvia, Riga, Latvia},
  author =        {Dobnik, Simon and Kelleher, John D. and
                   Ghanimifard, Mehdi},
  institution =   {ESSLLI 2019, 31 European Summer School on Logic,
                   Language and Information},
  month =         {12--16 August},
  type =          {lecture notes},
  title =         {Language, Action and Perception ({APL-ESSLLI}):
                   Lecture Notes of a Course in Language and
                   Computation},
  year =          {2019},
  abstract =      {The course gives a survey of theory and practical
                   computational implementations of how natural language
                   interacts with the physical world through action and
                   perception. We will look at topics such as semantic
                   theories and computational approaches to modelling
                   natural language, action and perception (grounding),
                   situated dialogue systems, integrated robotic
                   systems, grounding of language in action and
                   perception, generation and interpretation of scene
                   descriptions from images and videos, spatial
                   cognition, and others. language, perception, action,
                   embodiment, spatial language, attention, grounding,
                   dialogue systems, robotic systems, learning language
                   with robots, image descriptions, generating referring
                   expressions, visual question answering, situated
                   interactive agents},
  url =           {https://gup.ub.gu.se/publication/283008?lang=en},
}

@inproceedings{Dobnik:2020aa,
  address =       {Cham, Switzerland},
  author =        {Dobnik, Simon and Ghanimifard, Mehdi},
  booktitle =     {Spatial Cognition XII: Proceedings of the 12th
                   International Conference, Spatial Cognition 2020,
                   Riga, Latvia},
  editor =        {{\v S}{\c k}ilters, Jur{\c g}is and Newcombe, Nora S. and
                   Uttal, David},
  pages =         {219--234},
  publisher =     {Springer International Publishing},
  title =         {Spatial descriptions on a functional-geometric
                   spectrum: the location of objects},
  year =          {2020},
  abstract =      {Experimental research on spatial descriptions shows
                   that their semantics are dependent on several
                   modalities, among others (i) a geometric
                   representation of space ("where", geometric
                   knowledge) and (ii) dynamic kinematic routines
                   between objects that are related ("what", functional
                   knowledge). In this paper we examine whether
                   geometric and functional bias of spatial relations is
                   also reflected in large corpora of images and their
                   corresponding descriptions. In particular, we examine
                   whether the variation in object locations in the
                   usage of a relation is a predictor of that relation's
                   functional or geometric bias. Previous experimental
                   psycho-linguistic work has examined the bias of some
                   spatial relations, however our corpus-based
                   computational analysis allows us to examine the bias
                   of spatial relations and verbs beyond those that have
                   been tested experimentally. Our findings have also
                   implications for building computational image
                   descriptions systems as we demonstrate what kind of
                   representational knowledge is required to model
                   spatial relations contained in them. spatial
                   descriptions, geometric, functional, corpus, image
                   captions, computational model},
  doi =           {10.1007/978-3-030-57983-8_17},
  url =           {https://gup.ub.gu.se/publication/294794?lang=en},
}

@inproceedings{Dobnik:2020ab,
  address =       {Cham, Switzerland},
  author =        {Dobnik, Simon and Kelleher, John D. and
                   Howes, Christine},
  booktitle =     {Spatial Cognition XII: Proceedings of the 12th
                   International Conference, Spatial Cognition 2020,
                   Riga, Latvia},
  editor =        {{\v S}{\c k}ilters, Jur{\c g}is and Newcombe, Nora S. and
                   Uttal, David},
  pages =         {251--267},
  publisher =     {Springer International Publishing},
  title =         {Local Alignment of Frame of Reference Assignment in
                   {E}nglish and {S}wedish Dialogue},
  year =          {2020},
  abstract =      {Experimental research on spatial descriptions shows
                   that their semantics are dependent on several
                   modalities, among others (i) a geometric
                   representation of space ("where", geometric
                   knowledge) and (ii) dynamic kinematic routines
                   between objects that are related ("what", functional
                   knowledge). In this paper we examine whether
                   geometric and functional bias of spatial relations is
                   also reflected in large corpora of images and their
                   corresponding descriptions. In particular, we examine
                   whether the variation in object locations in the
                   usage of a relation is a predictor of that relation's
                   functional or geometric bias. Previous experimental
                   psycho-linguistic work has examined the bias of some
                   spatial relations, however our corpus-based
                   computational analysis allows us to examine the bias
                   of spatial relations and verbs beyond those that have
                   been tested experimentally. Our findings have also
                   implications for building computational image
                   descriptions systems as we demonstrate what kind of
                   representational knowledge is required to model
                   spatial relations contained in them. spatial
                   descriptions, geometric, functional, corpus, image
                   captions, computational model},
  doi =           {10.1007/978-3-030-57983-8_17},
  url =           {https://gup.ub.gu.se/publication/294795?lang=en},
}

@inproceedings{Dobnik:2021uf,
  address =       {Potsdam, Germany},
  author =        {Dobnik, Simon and Silfversparre, Vera},
  booktitle =     {Proceedings of PotsDial - Semdial 2021: The 25th
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Breitholtz, Ellen and Georgila, Kallirroi and
                   Schlangen, David},
  month =         {September 20--22},
  pages =         {50--60},
  series =        {Proceedings (SemDial)},
  title =         {The red cup on the left: Reference, coreference and
                   attention in visual dialogue},
  year =          {2021},
  abstract =      {We examine how conversational partners refer, corefer
                   and direct attention in conversations over a visual
                   scene. Using an extension of the CoNLL annotation
                   scheme for coreference for the visual domain we
                   annotate the Swedish part of the Cups corpus. The
                   annotation consists of identifying noun phrases and
                   assigning them IDs of entities in the visual scene.
                   We perform quantitative and qualitative linguistic
                   analyses of the annotated data which point towards
                   interesting observations of how conversational
                   participants direct attention: it is likely that
                   entities are co-referred to within the same
                   conversational game, for spatial descriptions there
                   is a preference for lateral dimensions over front and
                   back and more attention is directed towards entities
                   that are visually ambiguous or those that are part of
                   the task. Overall, we demonstrate that referential
                   attention is driven by both visual and conceptual,
                   task-related information. reference, coreference,
                   attention, visual dialogue, corpus annotation,
                   Swedish},
  url =           {https://gup.ub.gu.se/publication/307645?lang=en},
}

@article{Dogan:2019aa,
  author =        {Fethiye Irmak Do{\u g}an and Sinan Kalkan and
                   Iolanda Leite},
  journal =       {arXiv},
  pages =         {1--8},
  title =         {Learning to Generate Unambiguous Spatial Referring
                   Expressions for Real-World Environments},
  volume =        {arXiv:1904.07165 [cs.RO]},
  year =          {2019},
}

@inproceedings{Elliott:2018aa,
  address =       {Brussels, Belgium},
  author =        {Elliott, Desmond},
  booktitle =     {Proceedings of the 2018 Conference on Empirical
                   Methods in Natural Language Processing},
  month =         {October-November},
  pages =         {2974--2978},
  publisher =     {Association for Computational Linguistics},
  title =         {Adversarial Evaluation of Multimodal Machine
                   Translation},
  year =          {2018},
  url =           {http://www.aclweb.org/anthology/D18-1329},
}

@article{Foerster:2016aa,
  author =        {Jakob N. Foerster and Yannis M. Assael and
                   Nando de Freitas and Shimon Whiteson},
  journal =       {CoRR},
  title =         {Learning to Communicate with Deep Multi-Agent
                   Reinforcement Learning},
  volume =        {abs/1605.06676},
  year =          {2016},
}

@inproceedings{Forestier:2017aa,
  author =        {Forestier, S{\'e}bastien and Oudeyer, Pierre-Yves},
  booktitle =     {39th Annual Conference of the Cognitive Science
                   Society (CogSci 2017)},
  pages =         {2013--2018},
  title =         {A unified model of speech and tool use early
                   development},
  year =          {2017},
}

@inproceedings{Ghanimifard:2017ab,
  address =       {Montpellier, France},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of {IWCS 2017}: 12th International
                   Conference on Computational Semantics},
  editor =        {Gardent, Claire and Retor\'{e}, Christian},
  month =         {September 19--22},
  pages =         {1--12},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning to Compose Spatial Relations with Grounded
                   Neural Language Models},
  year =          {2017},
  abstract =      {Language is compositional: we can generate and
                   interpret novel sentences by having a notion of the
                   meaning of their individual parts. Spatial
                   descriptions are grounded in perceptional
                   representations but their meaning is also defined by
                   what neighbouring words they co-occur with. In this
                   paper, we examine how language models conditioned on
                   perceptual features can capture the semantics of
                   composed phrases as well as of individual words. We
                   generate a synthetic dataset of spatial descriptions
                   referring to perceptual scenes and examine how
                   grounded language models built with deep neural
                   networks can account for compositionality of
                   descriptions -- by evaluating how the learned
                   language models can deal with novel grounded composed
                   descriptions and novel grounded decomposed
                   descriptions, constituents previously not seen in
                   isolation.},
  url =           {https://gup.ub.gu.se/publication/257763?lang=en},
}

@inproceedings{Ghanimifard:2018ab,
  address =       {Proceedings of the Workshop on Shortcomings in Vision
                   and Language (SiVL), ECCV 2018, Munich, Germany},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Computer Vision -- ECCV 2018 Workshops. ECCV 2018},
  editor =        {Leal-Taix{\'e}, L. and Roth, S.},
  pages =         {1--9},
  publisher =     {Springer, Cham},
  series =        {Lecture Notes in Computer Science (LNCS)},
  title =         {Knowing When to Look For What and Where: Evaluating
                   Generation of Spatial Descriptions with Adaptive
                   Attention},
  volume =        {11132},
  year =          {2018},
  doi =           {10.1007/978-3-030-11018-5_14},
  url =           {https://gup.ub.gu.se/publication/274350?lang=en},
}

@inproceedings{Ghanimifard:2019aa,
  address =       {Minneapolis, Minnesota, USA},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of the Combined Workshop on Spatial
                   Language Understanding (SpLU) and Grounded
                   Communication for Robotics (RoboNLP)},
  editor =        {Archna Bhatia and Yonatan Bisk and
                   Parisa Kordjamshidi and Jesse Thomason},
  month =         {6 June},
  organization =  {North American Chapter of the Association for
                   Computational Linguistics: Human Language
                   Technologies (NAACL-HLT 2019)},
  pages =         {71--81},
  publisher =     {Association for Computational Linguistics},
  title =         {{What} a neural language model tells us about spatial
                   relations},
  year =          {2019},
  abstract =      {Understanding and generating spatial descriptions
                   requires knowledge about what objects are related,
                   their functional interactions, and where the objects
                   are geometrically located. Different spatial
                   relations have different functional and geometric
                   bias. The wide usage of neural language models in
                   different areas including generation of image
                   description motivates the study of what kind of
                   knowledge is encoded in neural language models about
                   individual spatial relations. With the premise that
                   the functional bias of relations is expressed in
                   their word distributions, we construct multi-word
                   distributional vector representations and show that
                   these representations perform well on intrinsic
                   semantic reasoning tasks, thus confirming our
                   premise. A comparison of our vector representations
                   to human semantic judgments indicates that different
                   bias (functional or geometric) is captured in
                   different data collection tasks which suggests that
                   the contribution of the two meaning modalities is
                   dynamic, related to the context of the task. spatial
                   description, spatial language, neural language model,
                   vector representation, spatial reasoning},
  url =           {https://gup.ub.gu.se/publication/279635?lang=en},
}

@inproceedings{Ghanimifard:2019ab,
  address =       {Tokyo, Japan},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of the 12th International Conference on
                   Natural Language Generation (INLG-2019)},
  editor =        {van Deemter, Kees and Lin, Chenghua and
                   Takamura, Hiroya},
  month =         {29 October -- 1 November},
  organization =  {Association for Computational Linguistics},
  pages =         {1--15},
  title =         {What goes into a word: generating image descriptions
                   with top-down spatial knowledge},
  year =          {2019},
  abstract =      {Generating grounded image descriptions requires
                   associating linguistic units with their corresponding
                   visual clues. A common method is to train a decoder
                   language model with attention mechanism over
                   convolutional visual features. Attention weights
                   align the stratified visual features arranged by
                   their location with tokens, most commonly words, in
                   the target description. However, words such as
                   spatial relations (e.g. next to and under) are not
                   directly referring to geometric arrangements of
                   pixels but to complex geometric and conceptual
                   representations. The aim of this paper is to evaluate
                   what representations facilitate generating image
                   descriptions with spatial relations and lead to
                   better grounded language generation. In particular,
                   we investigate the contribution of four different
                   representational modalities in generating relational
                   referring expressions: (i) (pre-trained)
                   convolutional visual features, (ii) spatial attention
                   over visual features, (iii) top-down geometric
                   relational knowledge between objects, and (iv) world
                   knowledge captured by contextual embeddings in
                   language models. spatial descriptions grounded neural
                   language models attention representation learning},
  url =           {https://gup.ub.gu.se/publication/284052?lang=en},
}

@phdthesis{Ghanimifard:2020aa,
  address =       {Gothenburg, Sweden},
  author =        {Ghanimifard, Mehdi},
  month =         {May 27},
  school =        {Department of Philosophy, Linguistics and Theory of
                   Science, University of Gothenburg},
  type =          {Doctoral thesis},
  title =         {Why the pond is not outside the frog? Grounding in
                   contextual representations by neural language models},
  year =          {2020},
  abstract =      {The representation of written language semantics is a
                   central problem of language technology and a crucial
                   component of many natural language processing
                   applications, from part-of-speech tagging to text
                   summarization. These representations of linguistic
                   units, such as words or sentences, allow computer
                   applications that work with language to process and
                   manipulate the meaning of text. In particular, a
                   family of models has been successfully developed
                   based on automatically learning semantics from large
                   collections of text and embedding them into a vector
                   space, where semantic or lexical similarity is a
                   function of geometric distance. Co-occurrence
                   information of words in context is the main source of
                   data used to learn these representations. Such models
                   have typically been applied to learning
                   representations for word forms, which have been
                   widely applied, and proven to be highly successful,
                   as characterizations of semantics at the word level.
                   However, a word-level approach to meaning
                   representation implies that the different meanings,
                   or senses, of any polysemic word share one single
                   representation. This might be problematic when
                   individual word senses are of interest and explicit
                   access to their specific representations is required.
                   For instance, in cases such as an application that
                   needs to deal with word senses rather than word
                   forms, or when a digital lexicon's sense inventory
                   has to be mapped to a set of learned semantic
                   representations. In this thesis, we present a number
                   of models that try to tackle this problem by
                   automatically learning representations for word
                   senses instead of for words. In particular, we try to
                   achieve this by using two separate sources of
                   information: corpora and lexica for the Swedish
                   language. Throughout the five publications compiled
                   in this thesis, we demonstrate that it is possible to
                   generate word sense representations from these
                   sources of data individually and in conjunction, and
                   we observe that combining them yields superior
                   results in terms of accuracy and sense inventory
                   coverage. Furthermore, in our evaluation of the
                   different representational models proposed here, we
                   showcase the applicability of word sense
                   representations both to downstream natural language
                   processing applications and to the development of
                   existing linguistic resources.},
  url =           {http://hdl.handle.net/2077/64095},
}

@article{Glenberg:1997aa,
  author =        {Glenberg, Arthur M.},
  journal =       {The Behavioral and brain sciences},
  month =         {April},
  pages =         {1--55},
  title =         {What memory is for},
  volume =        {20},
  year =          {1997},
}

@book{Goebel:2013aa,
  author =        {Patrick Goebel},
  publisher =     {Lulu},
  title =         {{ROS} by example},
  year =          {2013},
}

@article{Gordon:2018vs,
  author =        {Daniel Gordon and Aniruddha Kembhavi and
                   Mohammad Rastegari and Joseph Redmon and Dieter Fox and
                   Ali Farhadi},
  journal =       {arXiv},
  title =         {IQA: Visual Question Answering in Interactive
                   Environments},
  volume =        {arXiv:1712.03316 [cs.CV]},
  year =          {2018},
  url =           {https://arxiv.org/abs/1712.03316},
}

@inproceedings{Goyal:2017aa,
  author =        {Yash Goyal and Tejas Khot and Douglas Summers{-}Stay and
                   Dhruv Batra and Devi Parikh},
  booktitle =     {Conference on Computer Vision and Pattern Recognition
                   (CVPR)},
  pages =         {1--11},
  title =         {Making the {V} in {VQA} Matter: Elevating the Role of
                   Image Understanding in {V}isual {Q}uestion
                   {A}nswering},
  year =          {2017},
}

@mastersthesis{Graaf:2016aa,
  address =       {Gothenburg, Sweden},
  author =        {de Graaf, Erik},
  month =         {June, 8th},
  note =          {{S}upervisor: Simon Dobnik, examiner: Richard
                   Johansson, opponent: Lorena Llozhi},
  school =        {Department of Philosophy, Linguistics and Theory of
                   Science. University of Gothenburg},
  title =         {Learning Objects and Spatial Relations with {K}inect},
  year =          {2016},
  abstract =      {In order for humans to have meaningful interactions
                   with a robotic system, this system should be capable
                   of grounding semantic representations to their
                   real-world representations, learn spatial
                   relationships and communicate using spoken human
                   language. End users need to be able to query the
                   system what objects it already has knowledge of, for
                   more efficient learning. Such systems exist, but
                   require large sample sizes, thus not allowing end
                   users to teach the system more objects when needed.
                   To overcome this problem, we developed a non-mobile
                   system dubbed Kille, that uses a 3D camera, SIFT
                   features and machine learning to allow a tutor to
                   teach the system objects and spatial relations. The
                   system is built upon the ROS (Robot Operating System)
                   framework and uses Opendial software as a dialogue
                   system, for which a ROS support was written as part
                   of this project. We describe the hardware of the
                   system, the software used and developed, and we
                   evaluate its performance. Our results show that Kille
                   performs well on small learning sets, considering the
                   low sample size it uses to learn. In contrast to
                   other approaches, we focus on learning by a tutor
                   presenting objects and not by providing a dataset.
                   Recognition of spatial relations works well, however
                   no definitive conclusions can be drawn. This is
                   largely due to the small number of participants and
                   the subjective nature of spatial relations.},
  url =           {http://hdl.handle.net/2077/66207},
}

@inproceedings{Greco:2019aa,
  address =       {Florence, Italy},
  author =        {Greco, Claudio and Plank, Barbara and
                   Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  booktitle =     {Proceedings of the 57th Annual Meeting of the
                   Association for Computational Linguistics},
  month =         jul,
  pages =         {3601--3605},
  publisher =     {Association for Computational Linguistics},
  title =         {Psycholinguistics Meets Continual Learning: Measuring
                   Catastrophic Forgetting in Visual Question Answering},
  year =          {2019},
  abstract =      {We study the issue of catastrophic forgetting in the
                   context of neural multimodal approaches to Visual
                   Question Answering (VQA). Motivated by evidence from
                   psycholinguistics, we devise a set of
                   linguistically-informed VQA tasks, which differ by
                   the types of questions involved (Wh-questions and
                   polar questions). We test what impact task difficulty
                   has on continual learning, and whether the order in
                   which a child acquires question types facilitates
                   computational models. Our results show that dramatic
                   forgetting is at play and that task difficulty and
                   order matter. Two well-known current continual
                   learning methods mitigate the problem only to a
                   limiting degree.},
  doi =           {10.18653/v1/P19-1350},
  url =           {https://www.aclweb.org/anthology/P19-1350},
}

@article{Hamilton:2014aa,
  author =        {Hamilton, Antonia F de C and Kessler, Klaus and
                   Creem-Regehr, Sarah H},
  journal =       {Frontiers in Human Neuroscience},
  month =         {June},
  number =        {403},
  pages =         {1--3},
  publisher =     {Frontiers Media S.A.},
  title =         {Perspective taking: building a neurocognitive
                   framework for integrating the ``social''and the
                   ``spatial''},
  volume =        {8},
  year =          {2014},
  doi =           {10.3389/fnhum.2014.00403},
  isbn =          {1662-5161},
  url =           {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4052522/},
}

@article{Harnad:1990,
  author =        {Harnad, Stevan},
  journal =       {Physica D},
  month =         {June},
  number =        {1--3},
  pages =         {335--346},
  title =         {The symbol grounding problem},
  volume =        {42},
  year =          {1990},
  doi =           {10.1016/0167-2789(90)90087-6},
}

@article{Hill:2017aa,
  author =        {Felix Hill and Karl Moritz Hermann and Phil Blunsom and
                   Stephen Clark},
  journal =       {arXiv},
  title =         {Understanding Grounded Language Learning Agents},
  volume =        {arXiv:1710.09867 [cs.CL]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1710.09867},
}

@article{Hill:2020aa,
  author =        {Felix Hill and Olivier Tieleman and Tamara von Glehn and
                   Nathaniel Wong and Hamza Merzic and Stephen Clark},
  journal =       {arXiv},
  month =         {October 14},
  pages =         {1--17},
  title =         {Grounded Language Learning Fast and Slow},
  volume =        {arXiv:2009.01719 [cs.CL]},
  year =          {2020},
  url =           {https://arxiv.org/abs/2009.01719},
}

@inproceedings{Hudson:2019aa,
  author =        {Hudson, Drew A and Manning, Christopher D},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {6700--6709},
  title =         {Gqa: A new dataset for real-world visual reasoning
                   and compositional question answering},
  year =          {2019},
  url =           {http://openaccess.thecvf.com/content_CVPR_2019/html/
                  Hudson_GQA_A_New_Dataset_for_Real-
  World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html},
}

@inproceedings{Ilinykh:2019aa,
  address =       {Tokyo, Japan},
  author =        {Ilinykh, Nikolai and Zarrie{\ss}, Sina and
                   Schlangen, David},
  booktitle =     {Proceedings of the 12th International Conference on
                   Natural Language Generation},
  month =         oct # {{--}} # nov,
  pages =         {152--157},
  publisher =     {Association for Computational Linguistics},
  title =         {Tell Me More: A Dataset of Visual Scene Description
                   Sequences},
  year =          {2019},
  abstract =      {We present a dataset consisting of what we call image
                   description sequences, which are multi-sentence
                   descriptions of the contents of an image. These
                   descriptions were collected in a pseudo-interactive
                   setting, where the describer was told to describe the
                   given image to a listener who needs to identify the
                   image within a set of images, and who successively
                   asks for more information. As we show, this setup
                   produced nicely structured data that, we think, will
                   be useful for learning models capable of planning and
                   realising such description discourses.},
  doi =           {10.18653/v1/W19-8621},
  url =           {https://www.aclweb.org/anthology/W19-8621},
}

@inproceedings{Ilinykh:2020aa,
  address =       {Dublin, Ireland},
  author =        {Ilinykh, Nikolai and Dobnik, Simon},
  booktitle =     {Proceedings of the 13th International Conference on
                   Natural Language Generation},
  editor =        {Brian Davis and Yvette Graham amd John Kelleher and
                   Yaji Sripada},
  month =         dec,
  pages =         {338--348},
  publisher =     {Association for Computational Linguistics},
  title =         {When an Image Tells a Story: The Role of Visual and
                   Semantic Information for Generating Paragraph
                   Descriptions},
  year =          {2020},
  abstract =      {Generating multi-sentence image descriptions is a
                   challenging task, which requires a good model to
                   produce coherent and accurate paragraphs, describing
                   salient objects in the image. We argue that multiple
                   sources of information are beneficial when describing
                   visual scenes with long sequences. These include (i)
                   perceptual information and (ii) semantic (language)
                   information about how to describe what is in the
                   image. We also compare the effects of using two
                   different pooling mechanisms on either a single
                   modality or their combination. We demonstrate that
                   the model which utilises both visual and language
                   inputs can be used to generate accurate and diverse
                   paragraphs when combined with a particular pooling
                   mechanism. The results of our automatic and human
                   evaluation show that learning to embed semantic
                   information along with visual stimuli into the
                   paragraph generation model is not trivial, raising a
                   variety of proposals for future experiments.},
  url =           {https://www.aclweb.org/anthology/2020.inlg-1.40},
}

@inproceedings{Ilinykh:2021wj,
  address =       {Groningen, Netherlands (Online)},
  author =        {Ilinykh, Nikolai and Dobnik, Simon},
  booktitle =     {Proceedings of the 1st Workshop on Multimodal
                   Semantic Representations (MMSR)},
  month =         {June},
  pages =         {45--55},
  publisher =     {Association for Computational Linguistics},
  title =         {How Vision Affects Language: Comparing Masked
                   Self-Attention in Uni-Modal and Multi-Modal
                   Transformer},
  year =          {2021},
  abstract =      {The problem of interpretation of knowledge learned by
                   multi-head self-attention in transformers has been
                   one of the central questions in NLP. However, a lot
                   of work mainly focused on models trained for
                   uni-modal tasks, e.g. machine translation. In this
                   paper, we examine masked self-attention in a
                   multi-modal transformer trained for the task of image
                   captioning. In particular, we test whether the
                   multi-modality of the task objective affects the
                   learned attention patterns. Our visualisations of
                   masked self-attention demonstrate that (i) it can
                   learn general linguistic knowledge of the textual
                   input, and (ii) its attention patterns incorporate
                   artefacts from visual modality even though it has
                   never accessed it directly. We compare our
                   transformer's attention patterns with masked
                   attention in distilgpt-2 tested for uni-modal text
                   generation of image captions. Based on the maps of
                   extracted attention weights, we argue that masked
                   self-attention in image captioning transformer seems
                   to be enhanced with semantic knowledge from images,
                   exemplifying joint language-and-vision information in
                   its attention patterns.},
  url =           {https://www.aclweb.org/anthology/2021.mmsr-1.5},
}

@article{Jordan:2005aa,
  author =        {Jordan, Pamela W and Walker, Marilyn A},
  journal =       {Journal of Artificial Intelligence Research},
  pages =         {157--194},
  title =         {Learning content selection rules for generating
                   object descriptions in dialogue},
  volume =        {24},
  year =          {2005},
  doi =           {http://dx.doi.org/10.1613/jair.1591},
}

@inproceedings{Karpathy:2015aa,
  author =        {Karpathy, Andrej and Fei-Fei, Li},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {3128--3137},
  title =         {Deep visual-semantic alignments for generating image
                   descriptions},
  year =          {2015},
}

@techreport{Karpathy:2015ab,
  author =        {Karpathy, Andrej and Fei-Fei, Li},
  institution =   {The Vision Lab, Stanford University},
  title =         {Automated Image Captioning with ConvNets and
                   Recurrent Nets},
  year =          {2015},
  url =           {https://cs.stanford.edu/people/karpathy/sfmltalk.pdf},
}

@article{Kelleher:2005fk,
  author =        {Kelleher, John D. and Costello, Fintan J. and
                   van Genabith, Josef},
  journal =       {Artificial Intelligence},
  number =        {1},
  pages =         {62--102},
  title =         {Dynamically Structuring Updating and Interrelating
                   Representations of Visual and Linguistic Discourse},
  volume =        {167},
  year =          {2005},
  abstract =      {The fundamental claim of this paper is that
                   salience---both visual and linguistic---is an
                   important overarching semantic category structuring
                   visually situated discourse. Based on this we argue
                   that computer systems attempting to model the
                   evolving context of a visually situated discourse
                   should integrate models of visual and linguistic
                   salience within their natural language processing
                   (NLP) framework. The paper highlights the importance
                   of dynamically updating and interrelating visual and
                   linguistic discourse context representations. To
                   support our approach, we have developed a real-time,
                   natural language virtual reality (NLVR) system
                   (called LIVE, for Linguistic Interaction with Virtual
                   Environments) that implements an NLP framework based
                   on both visual and linguistic salience. Within this
                   framework saliency information underpins two of the
                   core subtasks of NLP: reference resolution and the
                   generation of referring expressions. We describe the
                   theoretical basis and architecture of the LIVE NLP
                   framework and present extensive evaluation results
                   comparing the system's performance with that of human
                   participants in a number of experiments.},
  doi =           {https://doi.org/10.1016/j.artint.2005.04.008},
  issn =          {0004-3702},
  url =           {https://www.sciencedirect.com/science/article/pii/
                  S0004370205000974},
}

@article{Kelleher:2009fk,
  address =       {Cambridge, MA, USA},
  author =        {Kelleher, John D. and Costello, Fintan J.},
  journal =       {Computational Linguistics},
  number =        {2},
  pages =         {271--306},
  publisher =     {MIT Press},
  title =         {Applying computational models of spatial prepositions
                   to visually situated dialog},
  volume =        {35},
  year =          {2009},
  doi =           {10.1162/coli.06-78-prep14},
  issn =          {0891-2017},
}

@article{Kennington:2014aa,
  author =        {Kennington, Casey and Kousidis, Spyros and
                   Schlangen, David},
  journal =       {Proceedings of SIGdial 2014: Short Papers},
  title =         {{InproTKs:} A toolkit for incremental situated
                   processing},
  year =          {2014},
}

@inproceedings{Kennington:2015aa,
  address =       {Beijing, China},
  author =        {Kennington, Casey and Schlangen, David},
  booktitle =     {Proceedings of the 53rd Annual Meeting of the
                   Association for Computational Linguistics and the 7th
                   International Joint Conference on Natural Language
                   Processing (Volume 1: Long Papers)},
  month =         {July},
  pages =         {292--301},
  publisher =     {Association for Computational Linguistics},
  title =         {Simple Learning and Compositional Application of
                   Perceptually Grounded Word Meanings for Incremental
                   Reference Resolution},
  year =          {2015},
  url =           {http://www.aclweb.org/anthology/P15-1029},
}

@article{Kolve:2017aa,
  author =        {Eric Kolve and Roozbeh Mottaghi and Daniel Gordon and
                   Yuke Zhu and Abhinav Gupta and Ali Farhadi},
  journal =       {arXiv},
  title =         {{AI2-THOR:} An Interactive 3D Environment for Visual
                   {AI}},
  volume =        {arXiv:1712.05474 [cs.CV]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1712.05474},
}

@inproceedings{Krafka:2016aa,
  author =        {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and
                   Harini Kannan and Suchendra Bhandarkar and
                   Wojciech Matusik and Antonio Torralba},
  booktitle =     {IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  title =         {Eye Tracking for Everyone},
  year =          {2016},
  url =           {https://people.csail.mit.edu/khosla/papers/
                  cvpr2016_Khosla.pdf},
}

@article{Krahmer:2011aa,
  author =        {Krahmer, Emiel and van Deemter, Kees},
  journal =       {Computational Linguistics},
  number =        {1},
  pages =         {173--218},
  publisher =     {MIT Press},
  title =         {Computational Generation of Referring Expressions: A
                   Survey},
  volume =        {38},
  year =          {2011},
  isbn =          {0891-2017},
}

@inproceedings{Krause:2017aa,
  author =        {Jonathan Krause and Justin Johnson and Ranjay Krishna and
                   Li Fei-Fei},
  booktitle =     {2017 IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  month =         {July 21--26},
  pages =         {3337--3345},
  title =         {A Hierarchical Approach for Generating Descriptive
                   Image Paragraphs},
  year =          {2017},
  abstract =      {Recent progress on image captioning has made it
                   possible to generate novel sentences describing
                   images in natural language, but compressing an image
                   into a single sentence can describe visual content in
                   only coarse detail. While one new captioning
                   approach, dense captioning, can potentially describe
                   images in finer levels of detail by captioning many
                   regions within an image, it in turn is unable to
                   produce a coherent story for an image. In this paper
                   we overcome these limitations by generating entire
                   paragraphs for describing images, which can tell
                   detailed, unified stories. We develop a model that
                   decomposes both images and paragraphs into their
                   constituent parts, detecting semantic regions in
                   images and using a hierarchical recurrent neural
                   network to reason about language. Linguistic analysis
                   confirms the complexity of the paragraph generation
                   task, and thorough experiments on a new dataset of
                   image and paragraph pairs demonstrate the
                   effectiveness of our approach.},
  doi =           {10.1109/CVPR.2017.356},
  url =           {https://ieeexplore.ieee.org/abstract/document/8099839},
}

@article{Krishna:2016aa,
  author =        {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and
                   Johnson, Justin and Hata, Kenji and Kravitz, Joshua and
                   Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and
                   Shamma, David A and Bernstein, Michael and
                   Fei-Fei, Li},
  journal =       {International Journal of Computer Vision},
  month =         {May},
  number =        {1},
  pages =         {32--73},
  title =         {Visual {Genome}: Connecting Language and Vision Using
                   Crowdsourced Dense Image Annotations},
  volume =        {123},
  year =          {2017},
  doi =           {10.1007/s11263-016-0981-7},
  issn =          {1573-1405},
}

@incollection{Kruijff:2006ab,
  address =       {Berlin, Heidelberg},
  author =        {Kruijff, Geert-Jan M. and Kelleher, John D. and
                   Hawes, Nick},
  booktitle =     {Perception and Interactive Technologies.
                   International Tutorial and Research Workshop, PIT
                   2006 Kloster Irsee, Germany},
  editor =        {Andr{\'e}, Elisabeth and Dybkj{\ae}r, Laila and
                   Minker, Wolfgang and Neumann, Heiko and
                   Weber, Michael},
  pages =         {117--128},
  publisher =     {Springer},
  title =         {Information Fusion for Visual Reference Resolution in
                   Dynamic Situated Dialogue},
  year =          {2006},
}

@article{Kruijff:2007,
  author =        {Kruijff, Geert-Jan M. and Zender, Hendrik and
                   Jensfelt, Patric and Christensen, Henrik I.},
  journal =       {International Journal of Advanced Robotic Systems},
  note =          {Special issue on human and robot interactive
                   communication},
  number =        {1},
  pages =         {125--138},
  title =         {Situated dialogue and spatial organization: what,
                   where... and why?},
  volume =        {4},
  year =          {2007},
  url =           {http://www.cognitivesystems.org/publications/kruijff_etal07-
                  jars.pdf},
}

@inproceedings{Kulkarni:2011aa,
  author =        {G. {Kulkarni} and V. {Premraj} and S. {Dhar} and
                   S. {Li} and Y. {Choi} and A. C. {Berg} and
                   T. L. {Berg}},
  booktitle =     {CVPR 2011},
  month =         {June},
  pages =         {1601--1608},
  title =         {Baby talk: Understanding and generating simple image
                   descriptions},
  year =          {2011},
  abstract =      {We posit that visually descriptive language offers
                   computer vision researchers both information about
                   the world, and information about how people describe
                   the world. The potential benefit from this source is
                   made more significant due to the enormous amount of
                   language data easily available today. We present a
                   system to automatically generate natural language
                   descriptions from images that exploits both
                   statistics gleaned from parsing large quantities of
                   text data and recognition algorithms from computer
                   vision. The system is very effective at producing
                   relevant sentences for images. It also generates
                   descriptions that are notably more true to the
                   specific image content than previous work.},
  doi =           {10.1109/CVPR.2011.5995466},
  issn =          {1063-6919},
  url =           {https://ieeexplore.ieee.org/document/5995466/
                  authors#authors},
}

@mastersthesis{Lang:2011aa,
  address =       {Berlin, Germany},
  author =        {Simon Lang},
  school =        {Institut f{\"u}r Informatik, Freie Universit{\"a}t
                   Berlin},
  type =          {Bachelorarbeit},
  title =         {Sign Language Recognition with Kinect},
  year =          {2011},
}

@inproceedings{Lang:2012aa,
  address =       {Berlin, Heidelberg},
  author =        {Lang, Simon and Block, Marco and Rojas, Ra{\'u}l},
  booktitle =     {Artificial Intelligence and Soft Computing: 11th
                   International Conference, ICAISC 2012, Zakopane,
                   Poland, April 29-May 3, 2012, Proceedings, Part I},
  editor =        {Rutkowski, Leszek and Korytkowski, Marcin and
                   Scherer, Rafa{\l} and Tadeusiewicz, Ryszard and
                   Zadeh, Lotfi A. and Zurada, Jacek M.},
  pages =         {394--402},
  publisher =     {Springer Berlin Heidelberg},
  title =         {Sign Language Recognition Using Kinect},
  year =          {2012},
  abstract =      {An open source framework for general gesture
                   recognition is presented and tested with isolated
                   signs of sign language. Other than common systems for
                   sign language recognition, this framework makes use
                   of Kinect, a depth camera which makes real-time
                   3D-reconstruction easily applicable. Recognition is
                   done using hidden Markov models with a continuous
                   observation density. The framework also offers an
                   easy way of initializing and training new gestures or
                   signs by performing them several times in front of
                   the camera. First results with a recognition rate of
                   97{\%} show that depth cameras are well-suited for
                   sign language recognition.},
  doi =           {10.1007/978-3-642-29347-4_46},
  isbn =          {978-3-642-29347-4},
}

@article{Lauria:2001,
  author =        {Lauria, Stanislao and Bugmann, Guido and
                   Kyriacou, Theocharis and Bos, Johan and Klein, Ewan},
  journal =       {IEEE Intelligent Systems},
  month =         {September/October},
  pages =         {38--45},
  title =         {Training personal robots using natural language
                   instruction},
  volume =        {16},
  year =          {2001},
}

@article{LauriaEtAl:2002b,
  author =        {Lauria, Stanislao and Bugmann, Guido and
                   Kyriacou, Theocharis and Klein, Ewan},
  journal =       {Robotics and Autonomous Systems},
  number =        {3--4},
  pages =         {171--181},
  title =         {Mobile robot programming using natural language},
  volume =        {38},
  year =          {2002},
  abstract =      {How will naive users program domestic robots? This
                   paper describes the design of a practical system that
                   uses natural language to teach a vision-based robot
                   how to navigate in a miniature town. To enable
                   unconstrained speech the robot is provided with a set
                   of primitive procedures derived from a corpus of
                   route instructions. When the user refers to a route
                   that is not known to the robot, the system will learn
                   it by combining primitives as instructed by the user.
                   This paper describes the components of the
                   Instruction-Based Learning architecture and discusses
                   issues of knowledge representation, the selection of
                   primitives and the conversion of natural language
                   into robot-understandable procedures.},
  doi =           {http://dx.doi.org/10.1016/S0921-8890(02)00166-5},
}

@article{Lazaridou:2016aa,
  author =        {Angeliki Lazaridou and Alexander Peysakhovich and
                   Marco Baroni},
  journal =       {arXiv},
  pages =         {1--11},
  title =         {Multi-Agent Cooperation and the Emergence of
                   (Natural) Language},
  volume =        {arXiv:1612.07182v2 [cs.CL]},
  year =          {2016},
  url =           {http://arxiv.org/abs/1612.07182},
}

@inproceedings{Lin:2014vm,
  address =       {Cham},
  author =        {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and
                   Hays, James and Perona, Pietro and Ramanan, Deva and
                   Doll{\'a}r, Piotr and Zitnick, C. Lawrence},
  booktitle =     {Computer Vision -- ECCV 2014},
  editor =        {Fleet, David and Pajdla, Tomas and Schiele, Bernt and
                   Tuytelaars, Tinne},
  pages =         {740--755},
  publisher =     {Springer International Publishing},
  title =         {Microsoft COCO: Common Objects in Context},
  year =          {2014},
  abstract =      {We present a new dataset with the goal of advancing
                   the state-of-the-art in object recognition by placing
                   the question of object recognition in the context of
                   the broader question of scene understanding. This is
                   achieved by gathering images of complex everyday
                   scenes containing common objects in their natural
                   context. Objects are labeled using per-instance
                   segmentations to aid in precise object localization.
                   Our dataset contains photos of 91 objects types that
                   would be easily recognizable by a 4 year old. With a
                   total of 2.5 million labeled instances in 328k
                   images, the creation of our dataset drew upon
                   extensive crowd worker involvement via novel user
                   interfaces for category detection, instance spotting
                   and instance segmentation. We present a detailed
                   statistical analysis of the dataset in comparison to
                   PASCAL, ImageNet, and SUN. Finally, we provide
                   baseline performance analysis for bounding box and
                   segmentation detection results using a Deformable
                   Parts Model.},
  isbn =          {978-3-319-10602-1},
  url =           {https://link.springer.com/chapter/10.1007/978-3-319-10602-
                  1_48},
}

@inproceedings{Loaiciga:2021aa,
  address =       {Online},
  author =        {Lo{\'a}iciga, Sharid and Dobnik, Simon and
                   Schlangen, David},
  booktitle =     {Proceedings of the Second Workshop on Advances in
                   Language and Vision Research},
  month =         jun,
  pages =         {39--44},
  publisher =     {Association for Computational Linguistics},
  title =         {Reference and coreference in situated dialogue},
  year =          {2021},
  abstract =      {In recent years several corpora have been developed
                   for vision and language tasks. We argue that there is
                   still significant room for corpora that increase the
                   complexity of both visual and linguistic domains and
                   which capture different varieties of perceptual and
                   conversational contexts. Working with two corpora
                   approaching this goal, we present a linguistic
                   perspective on some of the challenges in creating and
                   extending resources combining language and vision
                   while preserving continuity with the existing best
                   practices in the area of coreference annotation.},
  doi =           {10.18653/v1/2021.alvr-1.7},
  url =           {https://aclanthology.org/2021.alvr-1.7},
}

@inproceedings{Loaiciga:2021uy,
  address =       {Online},
  author =        {Lo{\'a}iciga, Sharid and Dobnik, Simon and
                   Schlangen, David},
  booktitle =     {Proceedings of the Second Workshop on Advances in
                   Language and Vision Research},
  month =         jun,
  pages =         {39--44},
  publisher =     {Association for Computational Linguistics},
  title =         {Reference and coreference in situated dialogue},
  year =          {2021},
  abstract =      {In recent years several corpora have been developed
                   for vision and language tasks. We argue that there is
                   still significant room for corpora that increase the
                   complexity of both visual and linguistic domains and
                   which capture different varieties of perceptual and
                   conversational contexts. Working with two corpora
                   approaching this goal, we present a linguistic
                   perspective on some of the challenges in creating and
                   extending resources combining language and vision
                   while preserving continuity with the existing best
                   practices in the area of coreference annotation.},
  doi =           {10.18653/v1/2021.alvr-1.7},
  url =           {https://aclanthology.org/2021.alvr-1.7},
}

@incollection{LoganSadler:1996,
  address =       {Cambridge, MA},
  author =        {Logan, Gordon D. and Sadler, Daniel D.},
  booktitle =     {Language and Space},
  editor =        {Bloom, Paul and Peterson, Mary A. and Nadel, Lynn and
                   Garrett, Merrill F.},
  pages =         {493--530},
  publisher =     {MIT Press},
  title =         {A computational analysis of the apprehension of
                   spatial relations},
  year =          {1996},
}

@inproceedings{Lowe:1999aa,
  author =        {Lowe, David G},
  booktitle =     {Computer vision, 1999. The proceedings of the seventh
                   IEEE international conference on},
  organization =  {IEEE},
  pages =         {1150--1157},
  title =         {Object recognition from local scale-invariant
                   features},
  volume =        {2},
  year =          {1999},
  doi =           {10.1109/ICCV.1999.790410},
}

@article{Lowe:2004aa,
  author =        {Lowe, David G},
  journal =       {International journal of computer vision},
  number =        {2},
  pages =         {91--110},
  publisher =     {Springer},
  title =         {Distinctive image features from scale-invariant
                   keypoints},
  volume =        {60},
  year =          {2004},
}

@unpublished{Lu:2016aa,
  author =        {Jiasen Lu and Caiming Xiong and Devi Parikh and
                   Richard Socher},
  month =         {6 June},
  note =          {arXiv:1612.01887 [cs.CV]},
  title =         {Knowing When to Look: Adaptive Attention via A Visual
                   Sentinel for Image Captioning},
  year =          {2017},
  url =           {https://arxiv.org/abs/1612.01887},
}

@inproceedings{Malinowski:2015aa,
  author =        {Malinowski, Mateusz and Rohrbach, Marcus and
                   Fritz, Mario},
  booktitle =     {Proceedings of the IEEE International Conference on
                   Computer Vision},
  pages =         {1--9},
  title =         {Ask your neurons: A neural-based approach to
                   answering questions about images},
  year =          {2015},
}

@phdthesis{Malinowski:2017aa,
  address =       {Saarbr{\"u}cken, Germany},
  author =        {Malinowski, Mateusz},
  month =         {June},
  school =        {Doctor of Engineering Science},
  type =          {Doctor of Engineering Science},
  title =         {Towards holistic machines : From visual recognition
                   to question answering about real-world images},
  year =          {2017},
  abstract =      {Computer Vision has undergone major changes over the
                   recent five years. Here, we investigate if the
                   performance of such architectures generalizes to more
                   complex tasks that require a more holistic approach
                   to scene comprehension. The presented work focuses on
                   learning spatial and multi-modal representations, and
                   the foundations of a Visual Turing Test, where the
                   scene understanding is tested by a series of
                   questions about its content. In our studies, we
                   propose DAQUAR, the first `question answering about
                   real-world images' dataset together with methods,
                   termed a symbolic-based and a neural-based visual
                   question answering architectures, that address the
                   problem. The symbolic-based method relies on a
                   semantic parser, a database of visual facts, and a
                   bayesian formulation that accounts for various
                   interpretations of the visual scene. The neural-based
                   method is an end-to-end architecture composed of a
                   question encoder, image encoder, multimodal
                   embedding, and answer decoder. This architecture has
                   proven to be effective in capturing language-based
                   biases. It also becomes the standard component of
                   other visual question answering architectures. Along
                   with the methods, we also investigate various
                   evaluation metrics that embraces uncertainty in
                   word's meaning, and various interpretations of the
                   scene and the question.},
  doi =           {doi:10.22028/D291-26773},
}

@article{Marge:2019aa,
  address =       {New York, NY, USA},
  author =        {Marge, Matthew and Rudnicky, Alexander I.},
  journal =       {ACM Trans. Interact. Intell. Syst.},
  month =         feb,
  number =        {1},
  pages =         {3:1--3:40},
  publisher =     {ACM},
  title =         {Miscommunication Detection and Recovery in Situated
                   Human\&\#x02013;Robot Dialogue},
  volume =        {9},
  year =          {2019},
  doi =           {10.1145/3237189},
  issn =          {2160-6455},
  url =           {http://doi.acm.org/10.1145/3237189},
}

@techreport{Marge:2019ab,
  address =       {Gothenburg, Sweden},
  author =        {Marge, Matthew},
  institution =   {Army Research Lab},
  month =         {September 9},
  type =          {Presentation in the CLASP seminar},
  title =         {Towards Natural Dialogue with Robots},
  year =          {2019},
  url =           {https://drive.google.com/file/d/1P3Zk88WVG37j9jDPxlV8zUGCea-
                  eDE6U/view},
}

@book{Marr:1982aa,
  author =        {Marr, David},
  month =         {August},
  publisher =     {MIT Press Scholarship Online},
  title =         {Vision: A computational approach},
  year =          {2010},
  annote =        {This posthumously published book (1982), which
                   influenced a generation of brain and cognitive
                   scientists, inspiring many to enter the field,
                   describes a general framework for understanding
                   visual perception and touches on broader questions
                   about how the brain and its functions can be studied
                   and understood. This MIT Press edition makes this
                   work available to a new generation of students and
                   scientists. In the author's framework, the process of
                   vision constructs a set of representations, starting
                   from a description of the input image and culminating
                   with a description of three-dimensional objects in
                   the surrounding environment. A central theme, and one
                   that has had far-reaching influence in both
                   neuroscience and cognitive science, is the notion of
                   different levels of analysis---in the author's
                   framework, the computational level, the algorithmic
                   level, and the hardware implementation level. Now,
                   thirty years later, the main problems that occupied
                   the author remain fundamental open problems in the
                   study of perception. His book provides inspiration
                   for the continuing efforts to integrate knowledge
                   from cognition and computation to understand vision
                   and the brain.},
  doi =           {10.7551/mitpress/9780262514620.001.0001},
}

@mastersthesis{Matsson:2018aa,
  address =       {Gothenburg, Sweden},
  author =        {Matsson, Arild},
  month =         {September 24},
  note =          {Examiner: Peter Ljungl{\"o}f; supervisors: Simon
                   Dobnik and Staffan Larsson; opponent: Axel Almqvist},
  school =        {Masters in Language Technology (MLT), Department of
                   Philosophy, Linguistics and Theory of Science.
                   University of Gothenburg},
  type =          {Masters in Language Technology (MLT), 30 HEC},
  title =         {Implementing perceptual semantics in Type Theory with
                   Records (TTR)},
  year =          {2018},
  url =           {http://hdl.handle.net/2077/62521},
}

@inproceedings{Matsson:2019aa,
  address =       {Gothenburg, Sweden},
  author =        {Matsson, Arild and Dobnik, Simon and
                   Larsson, Staffan},
  booktitle =     {Proceedings of the {IWCS} 2019 Workshop on Computing
                   Semantics with Types, Frames and Related Structures},
  editor =        {Rainer Osswald and Christian Retor{\'e} and
                   Peter Sutton},
  month =         {June},
  pages =         {55--64},
  publisher =     {Association for Computational Linguistics},
  title =         {{ImageTTR}: Grounding {T}ype {T}heory with {R}ecords
                   in Image Classification for Visual Question
                   Answering},
  year =          {2019},
  abstract =      {We present ImageTTR, an extension to the Python
                   implementation of Type Theory with Records (pyTTR)
                   which connects formal record type representation with
                   image classifiers implemented as deep neural
                   networks. The Type Theory with Records framework
                   serves as a knowledge representation system for
                   natural language the representations of which are
                   grounded in perceptual information of neural
                   networks. We demonstrate the benefits of this
                   symbolic and data-driven hybrid approach on the task
                   of visual question answering. Type Theory with
                   Records, image classification, knowledge
                   representation, visual question answering,
                   computational semantics},
  url =           {https://gup.ub.gu.se/publication/284011?lang=en},
}

@inproceedings{Matuszek:2012aa,
  address =       {Edinburgh, Scotland},
  author =        {Matuszek, Cynthia and FitzGerald, Nicholas and
                   Zettlemoyer, Luke and Bo, Liefeng and Fox, Dieter},
  booktitle =     {Proceedings of the 29th International Conference on
                   Machine Learning (ICML 2012)},
  editor =        {John Langford and Joelle Pineau},
  month =         {June 27th - July 3rd},
  title =         {A joint model of language and perception for grounded
                   attribute learning},
  year =          {2012},
}

@inproceedings{Matuszek:2012uq,
  author =        {Cynthia Matuszek and Evan Herbst and Luke Zettlemoyer and
                   Dieter Fox},
  booktitle =     {Proceedings of the 13th International Symposium on
                   Experimental Robotics (ISER)},
  month =         {June},
  title =         {Learning to Parse Natural Language Commands to a
                   Robot Control System},
  year =          {2012},
}

@article{McMahan:2015aa,
  author =        {McMahan, Brian and Stone, Matthew},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {103--115},
  title =         {A {B}ayesian model of grounded color semantics},
  volume =        {3},
  year =          {2015},
}

@techreport{McNerney:2011wu,
  author =        {McNerney, Samuel},
  institution =   {Scientific American},
  number =        {November 4},
  type =          {Guest blog},
  title =         {A Brief Guide to Embodied Cognition: Why You Are Not
                   Your Brain},
  year =          {2011},
  url =           {https://blogs.scientificamerican.com/guest-blog/a-brief-
                  guide-to-embodied-cognition-why-you-are-not-your-brain/},
}

@inproceedings{Mei:2016aa,
  author =        {Mei, Hongyuan and Bansal, Mohit and
                   Walter, Matthew R},
  booktitle =     {AAAI},
  pages =         {2772--2778},
  title =         {Listen, Attend, and Walk: Neural Mapping of
                   Navigational Instructions to Action Sequences},
  year =          {2016},
}

@techreport{Mohammed:2020vx,
  address =       {Gothenburg, Sweden},
  author =        {Mohammed, Yousuf Ali},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {February 3},
  note =          {{S}upervisor: Simon Dobnik and Mehdi Ghanimifard,
                   examiner: Staffan Larsson},
  type =          {Masters in Language Technology (MLT), 30 HEC},
  title =         {Guesswhat?! from what we answered before: Improving
                   the VQA task in goal-oriented games using the
                   previous context of dialogue Improving VQA task in
                   goal-oriented games using the previous context of the
                   dialogue},
  year =          {2020},
}

@article{Monroe:2016aa,
  author =        {Will Monroe and Noah D. Goodman and
                   Christopher Potts},
  journal =       {CoRR},
  title =         {Learning to Generate Compositional Color
                   Descriptions},
  volume =        {abs/1606.03821},
  year =          {2016},
  url =           {http://arxiv.org/abs/1606.03821},
}

@article{Monroe:2017ab,
  author =        {Monroe, Will and Hawkins, Robert X. D. and
                   Goodman, Noah D. and Potts, Christopher},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {325--338},
  title =         {Colors in Context: A Pragmatic Neural Model for
                   Grounded Language Understanding},
  volume =        {5},
  year =          {2017},
  abstract =      {We present a model of pragmatic referring expression
                   interpretation in a grounded communication task
                   (identifying colors from descriptions) that draws
                   upon predictions from two recurrent neural network
                   classifiers, a speaker and a listener, unified by a
                   recursive pragmatic reasoning framework. Experiments
                   show that this combined pragmatic model interprets
                   color descriptions more accurately than the
                   classifiers from which it is built, and that much of
                   this improvement results from combining the speaker
                   and listener perspectives. We observe that pragmatic
                   reasoning helps primarily in the hardest cases: when
                   the model must distinguish very similar colors, or
                   when few utterances adequately express the target
                   color. Our findings make use of a newly-collected
                   corpus of human utterances in color reference games,
                   which exhibit a variety of pragmatic behaviors. We
                   also show that the embedded speaker model reproduces
                   many of these pragmatic behaviors.},
  issn =          {2307-387X},
  url =           {https://transacl.org/ojs/index.php/tacl/article/view/1142},
}

@phdthesis{Monroe:2018aa,
  author =        {Monroe, Will},
  month =         {June},
  school =        {Department of Computer Science, Stanford University},
  type =          {Doctor of Philosophy},
  title =         {Learning in the rational speech acts model},
  year =          {2018},
  url =           {https://wmonroeiv.github.io/pubs/dissertation.pdf},
}

@article{Muja:2009aa,
  author =        {Muja, Marius and Lowe, David G},
  journal =       {VISAPP (1)},
  number =        {331--340},
  pages =         {2},
  title =         {Fast approximate nearest neighbors with automatic
                   algorithm configuration},
  volume =        {2},
  year =          {2009},
}

@incollection{Mukerjee:1998,
  address =       {Mahwah, N.J.},
  author =        {Mukerjee, Amitabha},
  booktitle =     {Representation and Processing of Spatial Expressions},
  editor =        {Olivier, Patrick and Gapp, Klaus-Peter},
  pages =         {1--36},
  publisher =     {Lawrence Erlbaum Associates},
  title =         {Neat versus scruffy: a review of computational models
                   of spatial expressions},
  year =          {1998},
}

@inproceedings{Mustafa:2014aa,
  author =        {Mustafa, Edon and Dimopoulos, Konstantinos},
  booktitle =     {Dautov, R., Gkasis, P., \& Karama-nos, A. et
                   al.(2014). Proceedings of the 9th South East
                   EuropeanDoctoral Student Conference},
  organization =  {Thessaloniki: SEERC},
  pages =         {271--285},
  title =         {Sign Language Recognition using Kinect},
  year =          {2014},
}

@book{OKane:2013aa,
  author =        {Jason M. O'Kane},
  month =         {10},
  publisher =     {CreateSpace Independent Publishing Platform},
  title =         {A Gentle Introduction to ROS},
  year =          {2013},
  isbn =          {9781492143239},
}

@book{OlivierGapp:1998,
  address =       {Mahwah, N.J.},
  editor =        {Olivier, Patrick and Gapp, Klaus-Peter},
  publisher =     {Lawrence Erlbaum Associates},
  title =         {Representation and Processing of Spatial Expressions},
  year =          {1998},
}

@book{Oviatt:2017aa,
  address =       {New York, NY, USA},
  author =        {Oviatt, Sharon and Schuller, Bj\"{o}rn and
                   Cohen, Philip R. and Sonntag, Daniel and
                   Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
  publisher =     {Association for Computing Machinery and Morgan \&
                   Claypool},
  title =         {The Handbook of Multimodal-Multisensor Interfaces:
                   Foundations, User Modeling, and Common Modality
                   Combinations - Volume 1},
  volume =        {1},
  year =          {2017},
  isbn =          {978-1-97000-167-9},
}

@article{Pezzelle:2018aa,
  author =        {Sandro Pezzelle and Ionut{-}Teodor Sorodoc and
                   Raffaella Bernardi},
  journal =       {arXiv},
  pages =         {1--12},
  title =         {Comparatives, Quantifiers, Proportions: {A}
                   Multi-Task Model for the Learning of Quantities from
                   Vision},
  volume =        {arXiv:1804.05018 [cs.CV]},
  year =          {2018},
  url =           {http://arxiv.org/abs/1804.05018},
}

@article{Pezzulo:2011aa,
  author =        {Pezzulo, Giovanni and Barsalou, Lawrence and
                   Cangelosi, Angelo and Fischer, Martin and
                   Spivey, Michael and McRae, Ken},
  journal =       {Frontiers in Psychology},
  pages =         {5},
  title =         {The Mechanics of Embodiment: A Dialog on Embodiment
                   and Computational Modeling},
  volume =        {2},
  year =          {2011},
  abstract =      {Embodied theories are increasingly challenging
                   traditional views of cognition by arguing that
                   conceptual representations that constitute our
                   knowledge are grounded in sensory and motor
                   experiences, and processed at this sensorimotor
                   level, rather than being represented and processed
                   abstractly in an amodal conceptual system. Given the
                   established empirical foundation, and the relatively
                   underspecified theories to date, many researchers are
                   extremely interested in embodied cognition but are
                   clamouring for more mechanistic implementations. What
                   is needed at this stage is a push toward explicit
                   computational models that implement sensory-motor
                   grounding as intrinsic to cognitive processes. In
                   this article, six authors from varying backgrounds
                   and approaches address issues concerning the
                   construction of embodied computational models, and
                   illustrate what they view as the critical current and
                   next steps toward mechanistic theories of embodiment.
                   The first part has the form of a dialogue between two
                   fictional characters: Ernest, the ``experimenter'',
                   and Mary, the ``computational modeller''. The
                   dialogue consists of an interactive sequence of
                   questions, requests for clarification, challenges,
                   and (tentative) answers, and touches the most
                   important aspects of grounded theories that should
                   inform computational modeling and, conversely, the
                   impact that computational modeling could have on
                   embodied theories. The second part of the article
                   discusses the most important open challenges for
                   embodied computational modelling.},
  doi =           {10.3389/fpsyg.2011.00005},
  issn =          {1664-1078},
  url =           {https://www.frontiersin.org/article/10.3389/
                  fpsyg.2011.00005},
}

@techreport{Pustejovsky:2020aa,
  author =        {Pustejovsky, James and Krishnaswamy, Nikhil},
  institution =   {Department of Computer Science, Brandeis University},
  month =         {July},
  type =          {Journal article manuscript},
  title =         {Situated Meaning in Multimodal Dialogue: Human-Robot
                   and Human-Computer Interactions},
  year =          {2020},
  url =           {http://www.voxicon.net/wp-content/uploads/2020/07/TAL_2020-
                  13.pdf},
}

@inproceedings{Quigley:2009aa,
  author =        {Quigley, Morgan and Conley, Ken and Gerkey, Brian and
                   Faust, Josh and Foote, Tully and Leibs, Jeremy and
                   Wheeler, Rob and Ng, Andrew Y},
  booktitle =     {ICRA workshop on open source software},
  pages =         {5},
  title =         {{ROS:} an open-source Robot Operating System},
  volume =        {3},
  year =          {2009},
  url =           {http://www.ros.org/},
}

@inproceedings{Ramisa:2015aa,
  address =       {Lisbon, Portugal},
  author =        {Ramisa, Arnau and Wang, Josiah and Lu, Ying and
                   Dellandrea, Emmanuel and Moreno-Noguer, Francesc and
                   Gaizauskas, Robert},
  booktitle =     {Proceedings of the 2015 Conference on Empirical
                   Methods in Natural Language Processing},
  month =         {7--21 September},
  organization =  {Association for Computational Linguistics},
  pages =         {214--220},
  title =         {Combining geometric, textual and visual features for
                   predicting prepositions in image descriptions},
  year =          {2015},
  url =           {http://www.aclweb.org/anthology/D/D15/D15-1022.pdf},
}

@inproceedings{Rashtchian:2010kx,
  address =       {Los Angeles, CA},
  author =        {Cyrus Rashtchian and Peter Young and Micah Hodosh and
                   Julia Hockenmaier},
  booktitle =     {Proceedings of the {NAACL HLT} 2010 {W}orkshop on
                   creating speech and language data with {Amazon's
                   Mechanical Turk}},
  month =         {6 June},
  publisher =     {North American Chapter of the Association for
                   Computational Linguistics (NAACL)},
  title =         {Collecting Image Annotations Using {Amazon's
                   Mechanical Turk}},
  year =          {2010},
}

@book{Regier:1996,
  address =       {Cambridge, Massachusetts, London, England},
  author =        {Regier, Terry},
  publisher =     {MIT Press},
  title =         {The human semantic potential: spatial language and
                   constrained connectionism},
  year =          {1996},
}

@article{RegierCarlson:2001,
  author =        {Regier, Terry and Carlson, Laura A.},
  journal =       {Journal of Experimental Psychology: General},
  number =        {2},
  pages =         {273--298},
  title =         {Grounding spatial language in perception: an
                   empirical and computational investigation},
  volume =        {130},
  year =          {2001},
  doi =           {10.1037//0096-3445.130.2.273},
}

@inproceedings{Rennie:2017aa,
  author =        {Rennie, Steven J and Marcheret, Etienne and
                   Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {7008--7024},
  title =         {Self-critical sequence training for image captioning},
  year =          {2017},
}

@article{Retz-Schmidt:1988aa,
  author =        {Retz-Schmidt, Gudula},
  journal =       {AI magazine},
  number =        {2},
  pages =         {95--95},
  title =         {Various views on spatial prepositions},
  volume =        {9},
  year =          {1988},
  url =           {https://wvvw.aaai.org/ojs/index.php/aimagazine/article/
                  download/678/596},
}

@phdthesis{Rosman:2014aa,
  address =       {Edinburgh},
  author =        {Rosman, Benjamin Saul},
  school =        {Institute of Perception, Action and Behaviour, School
                   of Informatics, The University of Edinburgh},
  type =          {Doctor of Philosophy},
  title =         {Learning domain abstractions for long lived robots},
  year =          {2014},
  abstract =      {Recent trends in robotics have seen more general
                   purpose robots being deployed in unstructured
                   environments for prolonged periods of time. Such
                   robots are expected to adapt to different
                   environmental conditions, and ultimately take on a
                   broader range of responsibilities, the specifications
                   of which may change online after the robot has been
                   deployed. We propose that in order for a robot to be
                   generally capable in an online sense when it
                   encounters a range of unknown tasks, it must have the
                   ability to continually learn from a lifetime of
                   experience. Key to this is the ability to generalise
                   from experiences and form representations which
                   facilitate faster learning of new tasks, as well as
                   the transfer of knowledge between different
                   situations. However, experience cannot be managed
                   na¨ıvely: one does not want constantly expanding
                   tables of data, but instead continually refined
                   abstractions of the data -- much like humans seem to
                   abstract and organise knowledge. If this agent is
                   active in the same, or similar, classes of
                   environments for a prolonged period of time, it is
                   provided with the opportunity to build abstract
                   representations in order to simplify the learning of
                   future tasks. The domain is a common structure
                   underlying large families of tasks, and exploiting
                   this affords the agent the potential to not only
                   minimise relearning from scratch, but over time to
                   build better models of the environment. We propose to
                   learn such regularities from the environment, and
                   extract the commonalities between tasks. This thesis
                   aims to address the major question: what are the
                   domain invariances which should be learnt by a long
                   lived agent which encounters a range of different
                   tasks? This question can be decomposed into three
                   dimensions for learning invariances, based on
                   perception, action and interaction. We present novel
                   algorithms for dealing with each of these three
                   factors. Firstly, how does the agent learn to
                   represent the structure of the world? We focus here
                   on learning inter-object relationships from depth
                   information as a concise representation of the
                   structure of the domain. To this end we introduce
                   contact point networks as a topological abstraction
                   of a scene, and present an algorithm based on support
                   vector machine decision boundaries for extracting
                   these from three dimensional point clouds obtained
                   from the agent's experience of a domain. By reducing
                   the specific geometry of an environment into general
                   skeletons based on contact between different objects,
                   we can autonomously learn predicates describing
                   spatial relationships. Secondly, how does the agent
                   learn to acquire general domain knowledge? While the
                   agent attempts new tasks, it requires a mechanism to
                   control exploration, particularly when it has many
                   courses of action available to it. To this end we
                   draw on the fact that many local behaviours are
                   common to different tasks. Identifying these amounts
                   to learning ``common sense'' behavioural invariances
                   across multiple tasks. This principle leads to our
                   concept of action priors, which are defined as
                   Dirichlet distributions over the action set of the
                   agent. These are learnt from previous behaviours, and
                   expressed as the prior probability of selecting each
                   action in a state, and are used to guide the learning
                   of novel tasks as an exploration policy within a
                   reinforcement learning framework. Finally, how can
                   the agent react online with sparse information? There
                   are times when an agent is required to respond fast
                   to some interactive setting, when it may have
                   encountered similar tasks previously. To address this
                   problem, we introduce the notion of types, being a
                   latent class variable describing related problem
                   instances. The agent is required to learn, identify
                   and respond to these different types in online
                   interactive scenarios. We then introduce Bayesian
                   policy reuse as an algorithm that involves
                   maintaining beliefs over the current task instance,
                   updating these from sparse signals, and selecting and
                   instantiating an optimal response from a behaviour
                   library. This thesis therefore makes the following
                   contributions. We provide the first algorithm for
                   autonomously learning spatial relationships between
                   objects from point cloud data. We then provide an
                   algorithm for extracting action priors from a set of
                   policies, and show that considerable gains in speed
                   can be achieved in learning subsequent tasks over
                   learning from scratch, particularly in reducing the
                   initial losses associated with unguided exploration.
                   Additionally, we demonstrate how these action priors
                   allow for safe exploration, feature selection, and a
                   method for analysing and advising other agents'
                   movement through a domain. Finally, we introduce
                   Bayesian policy reuse which allows an agent to
                   quickly draw on a library of policies and instantiate
                   the correct one, enabling rapid online responses to
                   adversarial conditions.},
  url =           {http://hdl.handle.net/1842/9665},
}

@article{Roy:2002,
  author =        {Roy, Deb},
  journal =       {Computer speech and language},
  number =        {3},
  pages =         {353--385},
  title =         {Learning visually-grounded words and syntax for a
                   scene description task},
  volume =        {16},
  year =          {2002},
  abstract =      {A spoken language generation system has been
                   developed that learns to describe objects in
                   computer-generated visual scenes. The system is
                   trained by a show-and-tell' procedure in which visual
                   scenes are paired with natural language descriptions.
                   Learning algorithms acquire probabilistic structures
                   which encode the visual semantics of phrase
                   structure, word classes, and individual words. Using
                   these structures, a planning algorithm integrates
                   syntactic, semantic, and contextual constraints to
                   generate natural and unambiguous descriptions of
                   objects in novel scenes. The system generates
                   syntactically well-formed compound adjective noun
                   phrases, as well as relative spatial clauses. The
                   acquired linguistic structures generalize from
                   training data, enabling the production of novel word
                   sequences which were never observed during training.
                   The output of the generation system is synthesized
                   using word-based concatenative synthesis drawing from
                   the original training speech corpus. In evaluations
                   of semantic comprehension by human judges, the
                   performance of automatically generated spoken
                   descriptions was comparable to human-generated
                   descriptions. This work is motivated by our long-term
                   goal of developing spoken language processing systems
                   which grounds semantics in machine perception and
                   action.},
}

@article{Roy:2005,
  address =       {Essex, UK},
  author =        {Roy, Deb},
  journal =       {Artificial Intelligence},
  month =         {September},
  number =        {1-2},
  pages =         {170--205},
  publisher =     {Elsevier Science Publishers Ltd.},
  title =         {Semiotic schemas: a framework for grounding language
                   in action and perception},
  volume =        {167},
  year =          {2005},
  doi =           {10.1016/j.artint.2005.04.007},
  issn =          {0004-3702},
}

@techreport{Roy:2011aa,
  author =        {Roy, Deb},
  institution =   {TED: Ideas worth spreading},
  month =         {March},
  type =          {Talk},
  title =         {The birth of a word},
  year =          {2011},
  abstract =      {MIT researcher Deb Roy wanted to understand how his
                   infant son learned language -- so he wired up his
                   house with videocameras to catch every moment (with
                   exceptions) of his son's life, then parsed 90,000
                   hours of home video to watch "gaaaa" slowly turn into
                   "water." Astonishing, data-rich research with deep
                   implications for how we learn.},
  url =           {https://www.ted.com/talks/deb_roy_the_birth_of_a_word},
}

@phdthesis{Roy:2013aa,
  author =        {Roy, Brandon Cain},
  month =         {February},
  school =        {Program in Media Arts and Sciences, School of
                   Architecture and Planning, Massachusetts Institute of
                   Technology},
  type =          {Doctor of Philosophy in Media Arts and Sciences},
  title =         {The birth of a word},
  year =          {2013},
  abstract =      {A hallmark of a child's first two years of life is
                   their entry into language, from first productive word
                   use around 12 months of age to the emergence of
                   combinatorial speech in their second year. What is
                   the nature of early language development and how is
                   it shaped by everyday experience? This work builds
                   from the ground up to study early word learning,
                   characterizing vocabulary growth and its relation to
                   the child's environment. Our study is guided by the
                   idea that the natural activities and social
                   structures of daily life provide helpful learning
                   constraints. We study this through analysis of the
                   largest-ever corpus of one child's everyday
                   experience at home. Through the Human Speechome
                   Project, the home of a family with a young child was
                   outfitted with a custom audio-video recording system,
                   capturing more than 200,000 hours of audio and video
                   of daily life from birth to age three. The annotated
                   subset of this data spans the child's 9-24 month age
                   range and contains more than 8 million words of
                   transcribed speech, constituting a detailed record of
                   both the child's input and linguistic development.
                   Such a comprehensive, naturalistic dataset presents
                   new research opportunities but also requires new
                   analysis approaches " questions must be
                   operationalized to leverage the full scale of the
                   data. We begin with the task of speech transcription,
                   then identify "word births" " the child's first use
                   of each word in his vocabulary. Vocabulary growth
                   accelerates and then shows a surprising deceleration
                   that coincides with an increase in combinatorial
                   speech. The vocabulary growth timeline provides a
                   means to assess the environmental contributions to
                   word learning, beginning with aspects of caregiver
                   input speech. But language is tied to everyday
                   activity, and we investigate how spatial and activity
                   contexts relate to word learning. Activity contexts,
                   such as "mealtime", are identified manually and with
                   probabilistic methods that can scale to large
                   datasets. These new nonlinguistic variables are
                   predictive of when words are learned and are
                   complementary to more traditionally studied
                   linguistic measures. Characterizing word learning and
                   assessing natural input variables can lead to new
                   insights on fundamental learning mechanisms.},
  url =           {https://www.media.mit.edu/publications/the-birth-of-a-word/
                  },
}

@book{Russell:2016aa,
  author =        {Russell, Stuart J and Norvig, Peter and
                   Davis, Ernest},
  publisher =     {Pearson Education M.U.A.},
  series =        {Prentice Hall series in Artificial Intelligence},
  title =         {Artificial Intelligence: A Modern Approach},
  year =          {2016},
  isbn =          {1292153962},
}

@article{Savva:2019aa,
  author =        {Manolis Savva and Abhishek Kadian and
                   Oleksandr Maksymets and Yili Zhao and Erik Wijmans and
                   Bhavana Jain and Julian Straub and Jia Liu and
                   Vladlen Koltun and Jitendra Malik and Devi Parikh and
                   Dhruv Batra},
  journal =       {arXiv},
  pages =         {1--16},
  title =         {Habitat: A Platform for Embodied AI Research},
  volume =        {arXiv:1904.01201 [cs.CV]},
  year =          {2019},
  abstract =      {We present Habitat, a platform for research in
                   embodied artificial intelligence (AI). Habitat
                   enables training embodied agents (virtual robots) in
                   highly efficient photorealistic 3D simulation.
                   Specifically, Habitat consists of: (i) Habitat-Sim: a
                   flexible, high-performance 3D simulator with
                   configurable agents, sensors, and generic 3D dataset
                   handling. Habitat-Sim is fast -- when rendering a
                   scene from Matterport3D, it achieves several thousand
                   frames per second (fps) running single-threaded, and
                   can reach over 10,000 fps multi-process on a single
                   GPU. (ii) Habitat-API: a modular high-level library
                   for end-to-end development of embodied AI algorithms
                   -- defining tasks (e.g. navigation, instruction
                   following, question answering), configuring,
                   training, and benchmarking embodied agents. These
                   large-scale engineering contributions enable us to
                   answer scientific questions requiring experiments
                   that were till now impracticable or `merely'
                   impractical. Specifically, in the context of
                   point-goal navigation: (1) we revisit the comparison
                   between learning and SLAM approaches from two recent
                   works and find evidence for the opposite conclusion
                   -- that learning outperforms SLAM if scaled to an
                   order of magnitude more experience than previous
                   investigations, and (2) we conduct the first
                   cross-dataset generalization experiments {train,
                   test} × {Matterport3D, Gibson} for multiple sensors
                   {blind, RGB, RGBD, D} and find that only agents with
                   depth (D) sensors generalize across datasets. We hope
                   that our open-source platform and these findings will
                   advance research in embodied AI.},
  url =           {https://arxiv.org/abs/1904.01201},
}

@article{Scheutz:2011aa,
  author =        {Scheutz, Matthias and Cantrell, Rehj and
                   Schermerhorn, Paul},
  journal =       {AI Magazine},
  month =         {December},
  number =        {4},
  pages =         {77--84},
  title =         {Toward Humanlike Task-Based Dialogue Processing for
                   Human Robot Interaction},
  volume =        {32},
  year =          {2011},
  doi =           {10.1609/aimag.v32i4.2381},
  url =           {https://aaai.org/ojs/index.php/aimagazine/article/view/
                  2381},
}

@article{Scialom:2020aa,
  author =        {Thomas Scialom and Patrick Bordes and
                   Paul-Alexis Dray and Jacopo Staiano and
                   Patrick Gallinari},
  journal =       {arXiv},
  month =         {November 2},
  pages =         {1--11},
  title =         {What {BERT} Sees: Cross-Modal Transfer for Visual
                   Question Generation},
  volume =        {arXiv:2002.10832 [cs.CL]},
  year =          {2020},
  url =           {https://arxiv.org/abs/2002.10832},
}

@article{Shekhar:2019aa,
  author =        {Ravi Shekhar and Ece Takmaz and Raquel Fern{\'a}ndez and
                   Raffaella Bernardi},
  journal =       {arXiv},
  title =         {Evaluating the Representational Hub of Language and
                   Vision Models},
  volume =        {arXiv:1904.06038 [cs.CL]},
  year =          {2019},
  url =           {https://arxiv.org/abs/1904.06038},
}

@inproceedings{Shutova:2016aa,
  address =       {San Diego, California},
  author =        {Shutova, Ekaterina and Kiela, Douwe and
                   Maillard, Jean},
  booktitle =     {Proceedings of the 2016 Conference of the North
                   American Chapter of the Association for Computational
                   Linguistics: Human Language Technologies},
  month =         {June},
  pages =         {160--170},
  publisher =     {Association for Computational Linguistics},
  title =         {Black Holes and White Rabbits: Metaphor
                   Identification with Visual Features},
  year =          {2016},
  url =           {http://www.aclweb.org/anthology/N16-1020},
}

@inproceedings{Skantze:2012aa,
  author =        {Skantze, Gabriel and Al Moubayed, Samer},
  booktitle =     {Proceedings of the 14th ACM international conference
                   on Multimodal interaction},
  organization =  {ACM},
  pages =         {69--76},
  title =         {{IrisTK}: a statechart-based toolkit for multi-party
                   face-to-face interaction},
  year =          {2012},
}

@article{Skantze:2014aa,
  author =        {Skantze, Gabriel and Hjalmarsson, Anna and
                   Oertel, Catharine},
  journal =       {Speech Communication},
  pages =         {50--66},
  publisher =     {Elsevier},
  title =         {Turn-taking, feedback and joint attention in situated
                   human-robot interaction},
  volume =        {65},
  year =          {2014},
  abstract =      {In this paper, we present a study where a robot
                   instructs a human on how to draw a route on a map.
                   The human and robot are seated face-to-face with the
                   map placed on the table between them. The user's and
                   the robot's gaze can thus serve several simultaneous
                   functions: as cues to joint attention, turn-taking,
                   level of understanding and task progression. We have
                   compared this face-to-face setting with a setting
                   where the robot employs a random gaze behaviour, as
                   well as a voice-only setting where the robot is
                   hidden behind a paper board. In addition to this, we
                   have also manipulated turn-taking cues such as
                   completeness and filled pauses in the robot's speech.
                   By analysing the participants' subjective rating,
                   task completion, verbal responses, gaze behaviour,
                   and drawing activity, we show that the users indeed
                   benefit from the robot's gaze when talking about
                   landmarks, and that the robot's verbal and gaze
                   behaviour has a strong effect on the users'
                   turn-taking behaviour. We also present an analysis of
                   the users' gaze and lexical and prosodic realisation
                   of feedback after the robot instructions, and show
                   that these cues reveal whether the user has yet
                   executed the previous instruction, as well as the
                   user's level of uncertainty.},
}

@article{Skantze:2016aa,
  author =        {Skantze, Gabriel},
  journal =       {AI Magazine},
  number =        {4},
  pages =         {19--31},
  title =         {Real-time Coordination in Human-robot Interaction
                   using Face and Voice},
  volume =        {37},
  year =          {2016},
}

@inproceedings{Skocaj:2010fk,
  address =       {Anchorage, AK, USA},
  author =        {Danijel Sko\v{c}aj and Miroslav Jani\v{c}ek and
                   Matej Kristan and Geert-Jan M. Kruijff and
                   Ale\v{s} Leonardis and Pierre Lison and
                   Alen Vre\v{c}ko and Michael Zillich},
  booktitle =     {ICRA 2010 workshop ICAIR - Interactive Communication
                   for Autonomous Intelligent Robots},
  pages =         {30--36},
  title =         {A basic cognitive system for interactive continuous
                   learning of visual concepts},
  year =          {2010},
  abstract =      {Interactive continuous learning is an important
                   characteristic of a cognitive agent that is supposed
                   to operate and evolve in an everchanging environment.
                   In this paper we present representations and
                   mechanisms that are necessary for continuous learning
                   of visual concepts in dialogue with a tutor. We
                   present an approach for modelling beliefs stemming
                   from multiple modalities and we show how these
                   beliefs are created by processing visual and
                   linguistic information and how they are used for
                   learning. We also present a system that exploits
                   these representations and mechanisms, and demonstrate
                   these principles in the case of learning about object
                   colours and basic shapes in dialogue with the tutor.},
}

@inproceedings{Skocaj:2011fu,
  address =       {San Francisco, CA, USA},
  author =        {Danijel Sko\v{c}aj and Matej Kristan and
                   Alen Vre\v{c}ko and Marko Mahni\v{c} and
                   Miroslav Jan\'{i}\v{c}ek and Geert-Jan M. Kruijff and
                   Marc Hanheide and Nick Hawes and Thomas Keller and
                   Michael Zillich and Kai Zhou},
  booktitle =     {IEEE/RSJ International Conference on Intelligent
                   Robots and Systems IROS 2011},
  month =         {25-30 September},
  title =         {A system for interactive learning in dialogue with a
                   tutor},
  year =          {2011},
  abstract =      {In this paper we present representations and
                   mechanisms that facilitate continuous learning of
                   visual concepts in dialogue with a tutor and show the
                   implemented robot system. We present how beliefs
                   about the world are created by processing visual and
                   linguistic information and show how they are used for
                   planning system behaviour with the aim at satisfying
                   its internal drive -- to extend its knowledge. The
                   system facilitates different kinds of learning
                   initiated by the human tutor or by the system itself.
                   We demonstrate these principles in the case of
                   learning about object colours and basic shapes.},
  url =           {http://cogx.eu/data/cogx/publications/skocajIROS11.pdf},
}

@techreport{Specia:2020aa,
  address =       {Online},
  author =        {Specia, Lucia},
  institution =   {Imperial College London},
  month =         {July 9},
  type =          {Keynote talk at Advances in Language and Vision
                   Workshop at ACL 2020},
  title =         {Challenges in evaluating vision and language tasks},
  year =          {2020},
  url =           {https://slideslive.com/38929765/challenges-in-evaluating-
                  vision-and-language-tasks},
}

@article{Steels:2005os,
  author =        {Steels, Luc and Belpaeme, Tony},
  journal =       {Behavioral and Brain Sciences},
  number =        {4},
  pages =         {469-489},
  title =         {Coordinating Perceptually Grounded Categories Through
                   Language: A Case Study For Colour},
  volume =        {28},
  year =          {2005},
}

@book{Steels:2012aa,
  address =       {Amsterdam and Philadelphia},
  author =        {Steels, Luc},
  publisher =     {John Benjamins Pub. Co},
  series =        {Advances in interaction studies},
  title =         {Experiments in cultural language evolution},
  year =          {2012},
  url =           {https://gu-se-primo.hosted.exlibrisgroup.com/permalink/f/
                  rmbr1s/46GUB_KOHA1914065},
}

@article{SteelsBaillie:2003,
  author =        {Steels, Luc and Baillie, Jean-Christophe},
  journal =       {Robotics and Autonomous Systems},
  month =         {May},
  number =        {2--3},
  pages =         {163--173},
  title =         {Shared grounding of event descriptions by autonomous
                   robots},
  volume =        {43},
  year =          {2003},
  doi =           {doi:10.1016/S0921-8890(02)00357-3},
}

@incollection{SteelsLoetzsch:2009,
  author =        {Luc Steels and Martin Loetzsch},
  booktitle =     {Spatial Language and Dialogue},
  editor =        {Coventry, Kenny R. and Tenbrink, Thora and
                   Bateman, John. A.},
  publisher =     {Oxford University Press},
  title =         {Perspective Alignment in Spatial Language},
  year =          {2009},
}

@inproceedings{Stoia:2006zr,
  address =       {Sydney, Australia},
  author =        {Stoia, Laura and Shockley, Darla Magdalene and
                   Byron, Donna K. and Fosler-Lussier, Eric},
  booktitle =     {Proceedings of the Fourth International Natural
                   Language Generation Conference},
  month =         {July},
  pages =         {81--88},
  publisher =     {Association for Computational Linguistics},
  title =         {Noun Phrase Generation for Situated Dialogs},
  year =          {2006},
}

@techreport{Storckenfeldt:2018aa,
  address =       {Gothenburg, Sweden},
  author =        {Storckenfeldt, Axel},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {May 30},
  note =          {{S}upervisor: Simon Dobnik, examiner: Ylva Byrman},
  type =          {C-uppsats (Bachelor's thesis/extended essay)},
  title =         {Categorisation of conversational games in free
                   dialogue referring to spatial scenes},
  year =          {2018},
  abstract =      {This thesis examines which communicative strategies
                   speakers use to complete a given task regarding
                   spatial scenes, and how to systematically categorize
                   them as conversational games. This study expands a
                   free dialogue Cups corpus in Swedish (Dobnik et al.,
                   2016) from 794 to 985 turns and extend the existing
                   and new data with the annotation of conversational
                   games using a new Game Type Coding Scheme. The study
                   defines seven distinct types of conversational games
                   of which four show universal features believed to be
                   applicable across different spatial tasks. The study
                   shows some common features between the game types,
                   which would allow computational classification. The
                   reason why game types are important is that they
                   define a scope within which particular linguistic
                   features will be manifested (Dobnik et al.,2015).
                   These findings would be applicable on all situated
                   dialogue systems such as in cars, humanoids or
                   handheld devices in order to come one step closer to
                   more ``human-like'' answers to prompted questions
                   that involve spatial reasoning, such as, ``Siri,
                   where did I park my car?'' Computational Linguistics
                   Frame of Reference Spatial Description Conversational
                   Games Discourse Structure},
  url =           {http://hdl.handle.net/2077/58036},
}

@inproceedings{Takmaz:2020aa,
  address =       {Online},
  author =        {Takmaz, Ece and Giulianelli, Mario and
                   Pezzelle, Sandro and Sinclair, Arabella and
                   Fern{\'a}ndez, Raquel},
  booktitle =     {Proceedings of the 2020 Conference on Empirical
                   Methods in Natural Language Processing (EMNLP)},
  month =         nov,
  pages =         {4350--4368},
  publisher =     {Association for Computational Linguistics},
  title =         {{R}efer, {R}euse, {R}educe: {G}enerating {S}ubsequent
                   {R}eferences in {V}isual and {C}onversational
                   {C}ontexts},
  year =          {2020},
  abstract =      {Dialogue participants often refer to entities or
                   situations repeatedly within a conversation, which
                   contributes to its cohesiveness. Subsequent
                   references exploit the common ground accumulated by
                   the interlocutors and hence have several interesting
                   properties, namely, they tend to be shorter and reuse
                   expressions that were effective in previous mentions.
                   In this paper, we tackle the generation of first and
                   subsequent references in visually grounded dialogue.
                   We propose a generation model that produces referring
                   utterances grounded in both the visual and the
                   conversational context. To assess the referring
                   effectiveness of its output, we also implement a
                   reference resolution system. Our experiments and
                   analyses show that the model produces better, more
                   effective referring utterances than a model not
                   grounded in the dialogue context, and generates
                   subsequent references that exhibit linguistic
                   patterns akin to humans.},
  url =           {https://www.aclweb.org/anthology/2020.emnlp-main.353},
}

@article{Tellex:2011wf,
  author =        {Tellex, Stefanie and Kollar, Thomas and
                   Dickerson, Steven and Walter, Matthew and
                   Banerjee, Ashis and Teller, Seth and Roy, Nicholas},
  journal =       {Proceedings of the AAAI Conference on Artificial
                   Intelligence},
  month =         {Aug.},
  number =        {1},
  title =         {Understanding Natural Language Commands for Robotic
                   Navigation and Mobile Manipulation},
  volume =        {25},
  year =          {2011},
  abstract =      {This paper describes a new model for understanding
                   natural language commands given to autonomous systems
                   that perform navigation and mobile manipulation in
                   semi-structured environments. Previous approaches
                   have used models with fixed structure to infer the
                   likelihood of a sequence of actions given the
                   environment and the command. In contrast, our
                   framework, called Generalized Grounding Graphs,
                   dynamically instantiates a probabilistic graphical
                   model for a particular natural language command
                   according to the command's hierarchical and
                   compositional semantic structure. Our system performs
                   inference in the model to successfully find and
                   execute plans corresponding to natural language
                   commands such as put the tire pallet on the truck.
                   The model is trained using a corpus of commands
                   collected using crowdsourcing. We pair each command
                   with robot actions and use the corpus to learn the
                   parameters of the model. We evaluate the robot's
                   performance by inferring plans from natural language
                   commands, executing each plan in a realistic robot
                   simulator, and asking users to evaluate the system's
                   performance. We demonstrate that our system can
                   successfully follow many natural language commands
                   from the corpus.},
  url =           {https://ojs.aaai.org/index.php/AAAI/article/view/7979},
}

@techreport{Tellex:2020aa,
  author =        {Tellex, Stefanie},
  institution =   {Brown University},
  month =         {March 3},
  type =          {MIT CSAIL Embodied Intelligence Seminar},
  title =         {Towards Complex Language in Partially Observed
                   Environments},
  year =          {2020},
  url =           {https://youtu.be/zM_rrjwpMqw},
}

@article{Tenenbaum:2011ly,
  author =        {Tenenbaum, Joshua B. and Kemp, Charles and
                   Griffiths, Thomas L. and Goodman, Noah D.},
  journal =       {Science},
  number =        {6022},
  pages =         {1279-1285},
  title =         {How to Grow a Mind: Statistics, Structure, and
                   Abstraction},
  volume =        {331},
  year =          {2011},
  abstract =      {In coming to understand the world‚{\"A}{\^\i}in
                   learning concepts, acquiring language, and grasping
                   causal relations‚{\"A}{\^\i}our minds make
                   inferences that appear to go far beyond the data
                   available. How do we do it? This review describes
                   recent approaches to reverse-engineering human
                   learning and cognitive development and, in parallel,
                   engineering more humanlike machine learning systems.
                   Computational models that perform probabilistic
                   inference over hierarchies of flexibly structured
                   representations can address some of the deepest
                   questions about the nature and origins of human
                   thought: How does abstract knowledge guide learning
                   and reasoning from sparse data? What forms does our
                   knowledge take, across different domains and tasks?
                   And how is that abstract knowledge itself acquired?},
  doi =           {10.1126/science.1192788},
}

@techreport{Tenenbaum:2020aa,
  author =        {Tenenbaum, Joshua B.},
  institution =   {Center for Brains, Minds and Machines, MIT},
  month =         {July 5},
  type =          {ACL 2020 Keynote},
  title =         {Cognitive and computational building blocks for
                   morehuman-like language in machines},
  year =          {2020},
  url =           {https://slideslive.com/38929461/cognitive-and-computational-
                  building-blocks-for-more-humanlike-language-in-machines},
}

@inproceedings{Thomason:2019aa,
  author =        {Jesse Thomason and Michael Murray and Maya Cakmak and
                   Luke Zettlemoyer},
  booktitle =     {Conference on Robot Learning (CoRL)},
  title =         {Vision-and-Dialog Navigation},
  year =          {2019},
  url =           {https://arxiv.org/abs/1907.04957},
}

@techreport{Thomason:2019ab,
  author =        {Thomason, Jesse},
  institution =   {University of Washington},
  month =         {July 24},
  type =          {Microsoft Research Talks},
  title =         {Vision-and-Dialog Navigation (talk)},
  year =          {2019},
  abstract =      {Dialog-enabled smart assistants, which communicate
                   via natural language and occupy human homes, have
                   seen widespread adoption in recent years. These
                   systems can communicate information, but do not
                   manipulate objects or move themselves. By contrast,
                   manipulation-capable and mobile robots are still
                   largely deployed in industrial settings, but do not
                   interact with human users. Dialog-enabled robots can
                   bridge this gap, with natural language interfaces
                   helping robots and non-experts collaborate to achieve
                   their goals. In particular, navigation in unseen or
                   dynamic environments to high-level goals (e.g., ``Go
                   to the room with a plant'') can be facilitated by
                   enabling navigation agents to ask questions in
                   language, and to react to human clarifications
                   on-the-fly. To study this challenge, we introduce
                   Cooperative Vision-and-Dialog Navigation, an English
                   language dataset situated in the Matterport
                   Room-2-Room simulation environment.},
  url =           {https://www.microsoft.com/en-us/research/video/vision-and-
                  dialog-navigation/},
}

@inproceedings{Thomaz:2005aa,
  author =        {Thomaz, Andrea Lockerd and Berlin, Matt and
                   Breazeal, Cynthia},
  booktitle =     {ROMAN 2005. IEEE International Workshop on Robot and
                   Human Interactive Communication, 2005.},
  organization =  {IEEE},
  pages =         {591--598},
  title =         {An embodied computational model of social
                   referencing},
  year =          {2005},
  url =           {http://robotic.media.mit.edu/wp-content/uploads/sites/7/
                  2015/01/Lockerd_etal_RoMan-05.pdf},
}

@inproceedings{Utescher:2019aa,
  address =       {Gothenburg, Sweden},
  author =        {Utescher, Ronja},
  booktitle =     {Proceedings of the 13th International Conference on
                   Computational Semantics - Student Papers},
  month =         {23{--}27 } # may,
  pages =         {9--14},
  publisher =     {Association for Computational Linguistics},
  title =         {Visual {TTR} - Modelling Visual Question Answering in
                   Type Theory with Records},
  year =          {2019},
  abstract =      {In this paper, I will describe a system that was
                   developed for the task of Visual Question Answering.
                   The system uses the rich type universe of Type Theory
                   with Records (TTR) to model both the utterances about
                   the image, the image itself and classifications made
                   related to the two. At its most basic, the decision
                   of whether any given predicate can be assigned to an
                   object in the image is delegated to a CNN.
                   Consequently, images can be judged as evidence for
                   propositions. The end result is a model whose
                   application of perceptual classifiers to a given
                   image is guided by the accompanying utterance.},
  url =           {https://www.aclweb.org/anthology/W19-0602},
}

@unpublished{Vedaldi:2016aa,
  author =        {Vedaldi, Andrea},
  month =         {March},
  note =          {iV\&L summer school on vision and language, Malta.
  http://www.robots.ox.ac.uk/$\sim$vedaldi/assets/teach/vedaldi16deepcv.pdf},
  title =         {Convolutional Networks for Computer Vision
                   Applications},
  year =          {2016},
  url =           {http://www.robots.ox.ac.uk/~vedaldi/assets/teach/
                  vedaldi16deepcv.pdf},
}

@inproceedings{Vedantam:2015aa,
  author =        {Vedantam, Ramakrishna and Lawrence Zitnick, C. and
                   Parikh, Devi},
  booktitle =     {The IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  month =         {June},
  pages =         {4566--4575},
  title =         {CIDEr: Consensus-Based Image Description Evaluation},
  year =          {2015},
  abstract =      {Automatically describing an image with a sentence is
                   a long-standing challenge in computer vision and
                   natural language processing. Due to recent progress
                   in object detection, attribute classification, action
                   recognition, etc., there is renewed interest in this
                   area. However, evaluating the quality of descriptions
                   has proven to be challenging. We propose a novel
                   paradigm for evaluating image descriptions that uses
                   human consensus. This paradigm consists of three main
                   parts: a new triplet-based method of collecting human
                   annotations to measure consensus, a new automated
                   metric that captures consensus, and two new datasets:
                   PASCAL-50S and ABSTRACT-50S that contain 50 sentences
                   describing each image. Our simple metric captures
                   human judgment of consensus better than existing
                   metrics across sentences generated by various
                   sources. We also evaluate five state-of-the-art image
                   description approaches using this new protocol and
                   provide a benchmark for future comparisons. A version
                   of CIDEr named CIDEr-D is available as a part of MS
                   COCO evaluation server to enable systematic
                   evaluation and benchmarking.},
  url =           {https://www.cv-foundation.org/openaccess/content_cvpr_2015/
                  html/Vedantam_CIDEr_Consensus-
                  Based_Image_2015_CVPR_paper.html},
}

@inproceedings{Viethen:2008aa,
  author =        {Viethen, Jette and Dale, Robert},
  booktitle =     {Proceedings of the Fifth International Natural
                   Language Generation Conference},
  organization =  {Association for Computational Linguistics},
  pages =         {59--67},
  title =         {The use of spatial relations in referring expression
                   generation},
  year =          {2008},
}

@inproceedings{Viethen:2011aa,
  author =        {Viethen, Jette and Dale, Robert and Guhe, Markus},
  booktitle =     {Proceedings of the Conference on Empirical Methods in
                   Natural Language Processing},
  organization =  {Association for Computational Linguistics},
  pages =         {1158--1167},
  title =         {Generating subsequent reference in shared visual
                   scenes: Computation vs. re-use},
  year =          {2011},
}

@inproceedings{Viethen:2011ab,
  author =        {Viethen, Jette and Dale, Robert and Guhe, Markus},
  booktitle =     {Proceedings of the 13th European Workshop on Natural
                   Language Generation},
  organization =  {Association for Computational Linguistics},
  pages =         {44--52},
  title =         {The impact of visual context on the content of
                   referring expressions},
  year =          {2011},
}

@article{Wang:2016aa,
  author =        {Wang, Sida I and Liang, Percy and
                   Manning, Christopher D},
  journal =       {arXiv preprint arXiv:1606.02447},
  title =         {Learning Language Games through Interaction},
  year =          {2016},
}

@article{Winograd:1972,
  author =        {Winograd, Terry},
  journal =       {Cognitive Psychology},
  number =        {1},
  publisher =     {Edinburgh University Press},
  title =         {Understanding Natural Language},
  volume =        {3},
  year =          {1972},
}

@book{Winograd:1976,
  author =        {Winograd, Terry},
  publisher =     {Edinburgh University Press},
  title =         {Understanding Natural Language},
  year =          {1976},
}

@article{Xu:2015aa,
  author =        {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and
                   Courville, Aaron and Salakhutdinov, Ruslan and
                   Zemel, Richard and Bengio, Yoshua},
  journal =       {arXiv},
  month =         {February 11},
  pages =         {1--22},
  title =         {Show, attend and tell: Neural image caption
                   generation with visual attention},
  volume =        {arXiv:1502.03044 [cs.LG]},
  year =          {2015},
}

@article{Xu:2018aa,
  author =        {Xiaofeng Xu and Ivor W. Tsang and Chuancai Liu},
  journal =       {arXiv},
  title =         {Zero-shot Learning with Complementary Attributes},
  volume =        {arXiv:1804.06505 [cs.CV]},
  year =          {2018},
  url =           {http://arxiv.org/abs/1804.06505},
}

@article{Yang:2014aa,
  author =        {Yang, Hee-Deok},
  journal =       {Sensors},
  number =        {1},
  pages =         {135--147},
  publisher =     {Multidisciplinary Digital Publishing Institute},
  title =         {Sign language recognition with the kinect sensor
                   based on conditional random fields},
  volume =        {15},
  year =          {2014},
}

@article{Young:2014wj,
  author =        {Young, Peter and Lai, Alice and Hodosh, Micah and
                   Hockenmaier, Julia},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {67--78},
  publisher =     {MIT Press},
  title =         {From image descriptions to visual denotations: New
                   similarity metrics for semantic inference over event
                   descriptions},
  volume =        {2},
  year =          {2014},
  url =           {https://direct.mit.edu/tacl/article/doi/10.1162/
                  tacl_a_00166/43313/From-image-descriptions-to-visual-
                  denotations-New},
}

@article{Yuan:2016aa,
  author =        {Yuan, Lei and Uttal, David and Franconeri, Steven},
  journal =       {PloS one},
  month =         {October},
  number =        {10},
  pages =         {1--22},
  publisher =     {Public Library of Science},
  title =         {Are categorical spatial relations encoded by shifting
                   visual attention between objects?},
  volume =        {11},
  year =          {2016},
  url =           {https://doi.org/10.1371/journal.pone.0163141},
}

@article{Zaslavsky:2018aa,
  author =        {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and
                   Tishby, Naftali},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {31},
  pages =         {7937--7942},
  publisher =     {National Acad Sciences},
  title =         {Efficient compression in color naming and its
                   evolution},
  volume =        {115},
  year =          {2018},
}

@article{Zaslavsky:2019aa,
  author =        {Noga Zaslavsky and Terry Regier and Naftali Tishby and
                   Charles Kemp},
  journal =       {arXiv},
  title =         {Semantic categories of artifacts and animals reflect
                   efficient coding},
  volume =        {arXiv:1905.04562 [cs.CL]},
  year =          {2019},
  url =           {https://arxiv.org/abs/1905.04562},
}

@article{Zender:2008,
  author =        {Zender, Hendrik and {Mart\'{\i}nez-Mozos}, \'{O}scar and
                   Jensfelt, Patric and Kruijff, Geert-Jan M. and
                   Burgard, Wolfram},
  journal =       {Robotics and Autonomous Systems},
  month =         {June},
  note =          {Special issue ``From sensors to human spatial
                   concepts''},
  number =        {6},
  pages =         {493--502},
  title =         {Conceptual Spatial Representations for Indoor Mobile
                   Robots},
  volume =        {56},
  year =          {2008},
}

@article{Zender:2012uq,
  address =       {Los Alamitos, CA, USA},
  author =        {H. Zender and M. Janicek and Geert-Jan Kruijff},
  journal =       {IEEE Intelligent Systems},
  number =        {2},
  pages =         {27--35},
  publisher =     {IEEE Computer Society},
  title =         {Situated communication for joint activity in
                   human-robot teams},
  volume =        {27},
  year =          {2012},
  doi =               {http://doi.ieeecomputersociety.org/10.1109/MIS.2012.8},
  issn =          {1541-1672},
}

@inproceedings{Zhang:2019aa,
  author =        {Zhang, Ji and Kalantidis, Yannis and Rohrbach, Marcus and
                   Paluri, Manohar and Elgammal, Ahmed and
                   Elhoseiny, Mohamed},
  booktitle =     {Proceedings of the AAAI Conference on Artificial
                   Intelligence},
  pages =         {9185--9194},
  title =         {Large-scale visual relationship understanding},
  volume =        {33},
  year =          {2019},
  url =           {https://www.aaai.org/ojs/index.php/AAAI/article/view/4953/},
}

@inproceedings{Zhang:2019ab,
  author =        {Z. {Zhang} and Y. {Wang} and Q. {Wu} and F. {Chen}},
  booktitle =     {2019 International Joint Conference on Neural
                   Networks (IJCNN)},
  month =         {July},
  pages =         {1--8},
  title =         {Visual Relationship Attention for Image Captioning},
  year =          {2019},
  abstract =      {Visual attention mechanisms have been broadly used by
                   image captioning models to attend to related visual
                   information dynamically, allowing fine-grained image
                   understanding and reasoning. However, they are only
                   designed to discover the region-level alignment
                   between visual features and the language feature. The
                   exploration of higher-level visual relationship
                   information between image regions, which is rarely
                   researched in recent works, is beyond their
                   capabilities. To fill this gap, we propose a novel
                   visual relationship attention model based on the
                   parallel attention mechanism under the learnt spatial
                   constraints. It can extract relationship information
                   from visual regions and language and then achieve the
                   relationship-level alignment between them. Using
                   combined visual relationship attention and visual
                   region attention to attend to related visual
                   relationships and regions respectively, our image
                   captioning model can achieve state-of-the-art
                   performances on the MSCOCO dataset. Both quantitative
                   analysis and qualitative analysis demonstrate that
                   our novel visual relationship attention model can
                   capture related visual relationship and further
                   improve the caption quality.},
  doi =           {10.1109/IJCNN.2019.8851832},
  url =           {https://ieeexplore.ieee.org/abstract/document/8851832},
}

